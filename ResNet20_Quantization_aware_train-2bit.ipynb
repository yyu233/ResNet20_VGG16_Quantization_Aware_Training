{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "#model_name = \"VGG16_quant\"\n",
    "#model = VGG16_quant()\n",
    "model_name = \"resnet20_quant\"\n",
    "model = resnet20_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    #adjust_list = [150, 225]\n",
    "    adjust_list = [80, 120]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313a05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantConv2d(\n",
      "  3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.conv1 = QuantConv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "print(model.conv1)\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.bit = 2\n",
    "        layer.weight_quant = weight_quantize_fn(2)\n",
    "        layer.act_alq = act_quantization(2)\n",
    "        layer.act_alpha = torch.nn.Parameter(torch.tensor(4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c28df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.layer_type == \"QuantConv2d\":\n",
    "            print(layer.bit)\n",
    "            print(layer.weight_quant.w_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b02b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_Cifar(\n",
      "  (conv1): QuantConv2d(\n",
      "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (weight_quant): weight_quantize_fn()\n",
      "  )\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.817 (0.817)\tData 0.745 (0.745)\tLoss 2.5817 (2.5817)\tPrec 11.719% (11.719%)\n",
      "Epoch: [0][100/391]\tTime 0.043 (0.052)\tData 0.002 (0.009)\tLoss 1.4871 (1.6891)\tPrec 43.750% (38.428%)\n",
      "Epoch: [0][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.006)\tLoss 1.5641 (1.6262)\tPrec 46.094% (40.909%)\n",
      "Epoch: [0][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 1.5567 (1.5960)\tPrec 46.875% (42.112%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.460 (0.460)\tLoss 1.4798 (1.4798)\tPrec 50.000% (50.000%)\n",
      " * Prec 46.840% \n",
      "best acc: 46.840000\n",
      "Epoch: [1][0/391]\tTime 0.650 (0.650)\tData 0.587 (0.587)\tLoss 1.4545 (1.4545)\tPrec 42.969% (42.969%)\n",
      "Epoch: [1][100/391]\tTime 0.043 (0.052)\tData 0.002 (0.008)\tLoss 1.5685 (1.4578)\tPrec 45.312% (46.566%)\n",
      "Epoch: [1][200/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 1.4801 (1.4450)\tPrec 50.781% (47.322%)\n",
      "Epoch: [1][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 1.4710 (1.4346)\tPrec 46.875% (47.755%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.663 (0.663)\tLoss 1.6856 (1.6856)\tPrec 42.969% (42.969%)\n",
      " * Prec 42.000% \n",
      "best acc: 46.840000\n",
      "Epoch: [2][0/391]\tTime 0.767 (0.767)\tData 0.707 (0.707)\tLoss 1.3924 (1.3924)\tPrec 48.438% (48.438%)\n",
      "Epoch: [2][100/391]\tTime 0.050 (0.054)\tData 0.003 (0.009)\tLoss 1.2060 (1.3557)\tPrec 60.938% (51.470%)\n",
      "Epoch: [2][200/391]\tTime 0.038 (0.051)\tData 0.002 (0.005)\tLoss 1.2068 (1.3538)\tPrec 55.469% (51.232%)\n",
      "Epoch: [2][300/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 1.3522 (1.3479)\tPrec 55.469% (51.391%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.809 (0.809)\tLoss 1.4277 (1.4277)\tPrec 47.656% (47.656%)\n",
      " * Prec 49.800% \n",
      "best acc: 49.800000\n",
      "Epoch: [3][0/391]\tTime 0.438 (0.438)\tData 0.336 (0.336)\tLoss 1.3356 (1.3356)\tPrec 57.812% (57.812%)\n",
      "Epoch: [3][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.005)\tLoss 1.3030 (1.3052)\tPrec 56.250% (52.661%)\n",
      "Epoch: [3][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.004)\tLoss 1.4090 (1.2908)\tPrec 45.312% (53.063%)\n",
      "Epoch: [3][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 1.3999 (1.2835)\tPrec 54.688% (53.680%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 1.176 (1.176)\tLoss 1.4313 (1.4313)\tPrec 53.906% (53.906%)\n",
      " * Prec 51.930% \n",
      "best acc: 51.930000\n",
      "Epoch: [4][0/391]\tTime 0.846 (0.846)\tData 0.792 (0.792)\tLoss 1.3324 (1.3324)\tPrec 53.125% (53.125%)\n",
      "Epoch: [4][100/391]\tTime 0.043 (0.054)\tData 0.001 (0.010)\tLoss 1.1403 (1.2297)\tPrec 58.594% (55.376%)\n",
      "Epoch: [4][200/391]\tTime 0.052 (0.050)\tData 0.002 (0.006)\tLoss 1.1108 (1.2234)\tPrec 60.938% (55.546%)\n",
      "Epoch: [4][300/391]\tTime 0.041 (0.048)\tData 0.002 (0.005)\tLoss 1.1557 (1.2166)\tPrec 58.594% (55.988%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 1.1793 (1.1793)\tPrec 60.938% (60.938%)\n",
      " * Prec 54.190% \n",
      "best acc: 54.190000\n",
      "Epoch: [5][0/391]\tTime 0.487 (0.487)\tData 0.428 (0.428)\tLoss 1.2895 (1.2895)\tPrec 50.781% (50.781%)\n",
      "Epoch: [5][100/391]\tTime 0.041 (0.050)\tData 0.002 (0.006)\tLoss 1.1050 (1.1554)\tPrec 60.156% (58.509%)\n",
      "Epoch: [5][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 1.2198 (1.1564)\tPrec 63.281% (58.446%)\n",
      "Epoch: [5][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.3437 (1.1555)\tPrec 46.875% (58.560%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.377 (0.377)\tLoss 1.3807 (1.3807)\tPrec 46.875% (46.875%)\n",
      " * Prec 53.080% \n",
      "best acc: 54.190000\n",
      "Epoch: [6][0/391]\tTime 0.488 (0.488)\tData 0.431 (0.431)\tLoss 1.0142 (1.0142)\tPrec 60.938% (60.938%)\n",
      "Epoch: [6][100/391]\tTime 0.050 (0.057)\tData 0.002 (0.007)\tLoss 1.2145 (1.1304)\tPrec 57.812% (59.406%)\n",
      "Epoch: [6][200/391]\tTime 0.051 (0.054)\tData 0.002 (0.004)\tLoss 1.1338 (1.1176)\tPrec 58.594% (59.888%)\n",
      "Epoch: [6][300/391]\tTime 0.052 (0.051)\tData 0.002 (0.004)\tLoss 1.0173 (1.1133)\tPrec 58.594% (60.224%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.386 (0.386)\tLoss 1.2829 (1.2829)\tPrec 57.812% (57.812%)\n",
      " * Prec 53.640% \n",
      "best acc: 54.190000\n",
      "Epoch: [7][0/391]\tTime 0.493 (0.493)\tData 0.434 (0.434)\tLoss 1.0683 (1.0683)\tPrec 61.719% (61.719%)\n",
      "Epoch: [7][100/391]\tTime 0.053 (0.051)\tData 0.002 (0.006)\tLoss 1.0767 (1.0767)\tPrec 62.500% (62.175%)\n",
      "Epoch: [7][200/391]\tTime 0.045 (0.050)\tData 0.002 (0.004)\tLoss 1.2435 (1.0765)\tPrec 57.031% (61.746%)\n",
      "Epoch: [7][300/391]\tTime 0.046 (0.049)\tData 0.002 (0.003)\tLoss 1.0342 (1.0727)\tPrec 60.156% (61.823%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.9992 (0.9992)\tPrec 65.625% (65.625%)\n",
      " * Prec 60.910% \n",
      "best acc: 60.910000\n",
      "Epoch: [8][0/391]\tTime 0.500 (0.500)\tData 0.440 (0.440)\tLoss 1.0979 (1.0979)\tPrec 57.812% (57.812%)\n",
      "Epoch: [8][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.9218 (1.0566)\tPrec 66.406% (62.268%)\n",
      "Epoch: [8][200/391]\tTime 0.050 (0.049)\tData 0.002 (0.004)\tLoss 1.0704 (1.0451)\tPrec 60.156% (62.893%)\n",
      "Epoch: [8][300/391]\tTime 0.048 (0.049)\tData 0.002 (0.004)\tLoss 0.9789 (1.0408)\tPrec 68.750% (63.151%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.556 (0.556)\tLoss 1.0329 (1.0329)\tPrec 67.188% (67.188%)\n",
      " * Prec 61.260% \n",
      "best acc: 61.260000\n",
      "Epoch: [9][0/391]\tTime 1.069 (1.069)\tData 1.014 (1.014)\tLoss 0.9410 (0.9410)\tPrec 67.188% (67.188%)\n",
      "Epoch: [9][100/391]\tTime 0.045 (0.058)\tData 0.002 (0.012)\tLoss 1.0039 (1.0199)\tPrec 60.938% (63.560%)\n",
      "Epoch: [9][200/391]\tTime 0.048 (0.051)\tData 0.002 (0.007)\tLoss 1.0704 (1.0173)\tPrec 62.500% (63.849%)\n",
      "Epoch: [9][300/391]\tTime 0.042 (0.049)\tData 0.002 (0.005)\tLoss 1.0766 (1.0167)\tPrec 61.719% (63.915%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.629 (0.629)\tLoss 1.3231 (1.3231)\tPrec 60.156% (60.156%)\n",
      " * Prec 49.610% \n",
      "best acc: 61.260000\n",
      "Epoch: [10][0/391]\tTime 0.775 (0.775)\tData 0.719 (0.719)\tLoss 0.8931 (0.8931)\tPrec 68.750% (68.750%)\n",
      "Epoch: [10][100/391]\tTime 0.048 (0.056)\tData 0.002 (0.009)\tLoss 1.1192 (0.9892)\tPrec 57.812% (65.107%)\n",
      "Epoch: [10][200/391]\tTime 0.052 (0.050)\tData 0.002 (0.006)\tLoss 0.8848 (0.9900)\tPrec 64.062% (64.719%)\n",
      "Epoch: [10][300/391]\tTime 0.039 (0.049)\tData 0.002 (0.004)\tLoss 1.1130 (0.9874)\tPrec 59.375% (64.761%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.398 (0.398)\tLoss 1.4719 (1.4719)\tPrec 53.125% (53.125%)\n",
      " * Prec 50.220% \n",
      "best acc: 61.260000\n",
      "Epoch: [11][0/391]\tTime 0.565 (0.565)\tData 0.508 (0.508)\tLoss 0.9968 (0.9968)\tPrec 64.062% (64.062%)\n",
      "Epoch: [11][100/391]\tTime 0.047 (0.051)\tData 0.002 (0.007)\tLoss 0.9855 (0.9691)\tPrec 65.625% (66.019%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.8652 (0.9705)\tPrec 67.969% (65.784%)\n",
      "Epoch: [11][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 1.0613 (0.9684)\tPrec 60.156% (65.913%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.353 (0.353)\tLoss 1.0905 (1.0905)\tPrec 63.281% (63.281%)\n",
      " * Prec 62.050% \n",
      "best acc: 62.050000\n",
      "Epoch: [12][0/391]\tTime 0.700 (0.700)\tData 0.642 (0.642)\tLoss 1.0081 (1.0081)\tPrec 63.281% (63.281%)\n",
      "Epoch: [12][100/391]\tTime 0.045 (0.051)\tData 0.002 (0.008)\tLoss 0.8890 (0.9498)\tPrec 67.969% (66.638%)\n",
      "Epoch: [12][200/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.8351 (0.9448)\tPrec 71.875% (66.927%)\n",
      "Epoch: [12][300/391]\tTime 0.049 (0.047)\tData 0.001 (0.004)\tLoss 0.9884 (0.9487)\tPrec 65.625% (66.609%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 1.1127 (1.1127)\tPrec 64.062% (64.062%)\n",
      " * Prec 61.000% \n",
      "best acc: 62.050000\n",
      "Epoch: [13][0/391]\tTime 0.588 (0.588)\tData 0.530 (0.530)\tLoss 1.0060 (1.0060)\tPrec 61.719% (61.719%)\n",
      "Epoch: [13][100/391]\tTime 0.047 (0.053)\tData 0.002 (0.007)\tLoss 0.8564 (0.9219)\tPrec 72.656% (67.017%)\n",
      "Epoch: [13][200/391]\tTime 0.054 (0.049)\tData 0.002 (0.005)\tLoss 1.0897 (0.9376)\tPrec 57.031% (66.756%)\n",
      "Epoch: [13][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.9919 (0.9336)\tPrec 63.281% (66.884%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.450 (0.450)\tLoss 1.1370 (1.1370)\tPrec 62.500% (62.500%)\n",
      " * Prec 61.270% \n",
      "best acc: 62.050000\n",
      "Epoch: [14][0/391]\tTime 0.546 (0.546)\tData 0.491 (0.491)\tLoss 0.9886 (0.9886)\tPrec 62.500% (62.500%)\n",
      "Epoch: [14][100/391]\tTime 0.041 (0.051)\tData 0.002 (0.007)\tLoss 0.9663 (0.9214)\tPrec 65.625% (67.327%)\n",
      "Epoch: [14][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.9527 (0.9148)\tPrec 67.969% (67.669%)\n",
      "Epoch: [14][300/391]\tTime 0.047 (0.046)\tData 0.002 (0.004)\tLoss 1.0043 (0.9148)\tPrec 64.844% (67.790%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.473 (0.473)\tLoss 0.9915 (0.9915)\tPrec 65.625% (65.625%)\n",
      " * Prec 63.040% \n",
      "best acc: 63.040000\n",
      "Epoch: [15][0/391]\tTime 0.476 (0.476)\tData 0.422 (0.422)\tLoss 0.8636 (0.8636)\tPrec 64.062% (64.062%)\n",
      "Epoch: [15][100/391]\tTime 0.042 (0.048)\tData 0.002 (0.006)\tLoss 0.8777 (0.8869)\tPrec 67.188% (68.727%)\n",
      "Epoch: [15][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.004)\tLoss 0.7157 (0.8928)\tPrec 78.906% (68.389%)\n",
      "Epoch: [15][300/391]\tTime 0.041 (0.045)\tData 0.002 (0.003)\tLoss 0.9008 (0.9013)\tPrec 68.750% (68.130%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.457 (0.457)\tLoss 0.8667 (0.8667)\tPrec 67.188% (67.188%)\n",
      " * Prec 65.580% \n",
      "best acc: 65.580000\n",
      "Epoch: [16][0/391]\tTime 0.487 (0.487)\tData 0.429 (0.429)\tLoss 0.8121 (0.8121)\tPrec 75.781% (75.781%)\n",
      "Epoch: [16][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.006)\tLoss 0.8295 (0.8858)\tPrec 75.781% (68.564%)\n",
      "Epoch: [16][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.004)\tLoss 0.9073 (0.8859)\tPrec 65.625% (68.758%)\n",
      "Epoch: [16][300/391]\tTime 0.053 (0.045)\tData 0.003 (0.003)\tLoss 0.8338 (0.8874)\tPrec 70.312% (68.618%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.641 (0.641)\tLoss 0.7883 (0.7883)\tPrec 71.875% (71.875%)\n",
      " * Prec 64.740% \n",
      "best acc: 65.580000\n",
      "Epoch: [17][0/391]\tTime 0.858 (0.858)\tData 0.799 (0.799)\tLoss 0.9075 (0.9075)\tPrec 66.406% (66.406%)\n",
      "Epoch: [17][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.010)\tLoss 0.8362 (0.8670)\tPrec 65.625% (69.330%)\n",
      "Epoch: [17][200/391]\tTime 0.046 (0.050)\tData 0.002 (0.006)\tLoss 0.9618 (0.8735)\tPrec 63.281% (69.057%)\n",
      "Epoch: [17][300/391]\tTime 0.051 (0.050)\tData 0.002 (0.005)\tLoss 0.8706 (0.8746)\tPrec 67.188% (69.002%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.525 (0.525)\tLoss 0.8062 (0.8062)\tPrec 70.312% (70.312%)\n",
      " * Prec 66.760% \n",
      "best acc: 66.760000\n",
      "Epoch: [18][0/391]\tTime 0.935 (0.935)\tData 0.873 (0.873)\tLoss 0.7857 (0.7857)\tPrec 71.094% (71.094%)\n",
      "Epoch: [18][100/391]\tTime 0.042 (0.056)\tData 0.002 (0.011)\tLoss 0.7520 (0.8584)\tPrec 74.219% (69.910%)\n",
      "Epoch: [18][200/391]\tTime 0.048 (0.051)\tData 0.002 (0.006)\tLoss 0.7605 (0.8607)\tPrec 76.562% (69.881%)\n",
      "Epoch: [18][300/391]\tTime 0.047 (0.049)\tData 0.001 (0.005)\tLoss 0.9899 (0.8621)\tPrec 64.844% (69.838%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.9074 (0.9074)\tPrec 68.750% (68.750%)\n",
      " * Prec 67.520% \n",
      "best acc: 67.520000\n",
      "Epoch: [19][0/391]\tTime 0.620 (0.620)\tData 0.574 (0.574)\tLoss 0.8760 (0.8760)\tPrec 67.969% (67.969%)\n",
      "Epoch: [19][100/391]\tTime 0.042 (0.051)\tData 0.002 (0.008)\tLoss 0.7131 (0.8481)\tPrec 71.094% (70.289%)\n",
      "Epoch: [19][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.7701 (0.8548)\tPrec 74.219% (70.072%)\n",
      "Epoch: [19][300/391]\tTime 0.042 (0.047)\tData 0.001 (0.004)\tLoss 0.7674 (0.8510)\tPrec 68.750% (69.913%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.466 (0.466)\tLoss 0.8845 (0.8845)\tPrec 72.656% (72.656%)\n",
      " * Prec 67.400% \n",
      "best acc: 67.520000\n",
      "Epoch: [20][0/391]\tTime 0.514 (0.514)\tData 0.458 (0.458)\tLoss 0.7156 (0.7156)\tPrec 73.438% (73.438%)\n",
      "Epoch: [20][100/391]\tTime 0.042 (0.051)\tData 0.002 (0.007)\tLoss 0.7537 (0.8483)\tPrec 75.781% (70.289%)\n",
      "Epoch: [20][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.8058 (0.8449)\tPrec 73.438% (70.371%)\n",
      "Epoch: [20][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7475 (0.8380)\tPrec 77.344% (70.510%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 1.0520 (1.0520)\tPrec 66.406% (66.406%)\n",
      " * Prec 60.740% \n",
      "best acc: 67.520000\n",
      "Epoch: [21][0/391]\tTime 0.496 (0.496)\tData 0.405 (0.405)\tLoss 0.8758 (0.8758)\tPrec 67.969% (67.969%)\n",
      "Epoch: [21][100/391]\tTime 0.047 (0.051)\tData 0.002 (0.006)\tLoss 0.8334 (0.8316)\tPrec 71.875% (70.722%)\n",
      "Epoch: [21][200/391]\tTime 0.038 (0.048)\tData 0.002 (0.004)\tLoss 0.8271 (0.8356)\tPrec 74.219% (70.662%)\n",
      "Epoch: [21][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.003)\tLoss 0.8774 (0.8341)\tPrec 70.312% (70.580%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.361 (0.361)\tLoss 1.0843 (1.0843)\tPrec 64.062% (64.062%)\n",
      " * Prec 60.520% \n",
      "best acc: 67.520000\n",
      "Epoch: [22][0/391]\tTime 0.628 (0.628)\tData 0.572 (0.572)\tLoss 0.7471 (0.7471)\tPrec 76.562% (76.562%)\n",
      "Epoch: [22][100/391]\tTime 0.040 (0.050)\tData 0.002 (0.007)\tLoss 0.8253 (0.8230)\tPrec 68.750% (71.256%)\n",
      "Epoch: [22][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.005)\tLoss 1.0694 (0.8248)\tPrec 60.938% (71.070%)\n",
      "Epoch: [22][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.004)\tLoss 0.8628 (0.8177)\tPrec 75.000% (71.265%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.9678 (0.9678)\tPrec 67.188% (67.188%)\n",
      " * Prec 66.690% \n",
      "best acc: 67.520000\n",
      "Epoch: [23][0/391]\tTime 0.602 (0.602)\tData 0.546 (0.546)\tLoss 0.8408 (0.8408)\tPrec 70.312% (70.312%)\n",
      "Epoch: [23][100/391]\tTime 0.052 (0.050)\tData 0.003 (0.007)\tLoss 0.6766 (0.8156)\tPrec 75.781% (71.434%)\n",
      "Epoch: [23][200/391]\tTime 0.052 (0.048)\tData 0.003 (0.005)\tLoss 0.9303 (0.8192)\tPrec 65.625% (71.113%)\n",
      "Epoch: [23][300/391]\tTime 0.052 (0.048)\tData 0.002 (0.004)\tLoss 0.8750 (0.8119)\tPrec 65.625% (71.312%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 0.8752 (0.8752)\tPrec 71.094% (71.094%)\n",
      " * Prec 67.160% \n",
      "best acc: 67.520000\n",
      "Epoch: [24][0/391]\tTime 0.499 (0.499)\tData 0.442 (0.442)\tLoss 0.7350 (0.7350)\tPrec 69.531% (69.531%)\n",
      "Epoch: [24][100/391]\tTime 0.047 (0.052)\tData 0.002 (0.006)\tLoss 0.7421 (0.7974)\tPrec 75.000% (71.504%)\n",
      "Epoch: [24][200/391]\tTime 0.046 (0.049)\tData 0.001 (0.004)\tLoss 0.7791 (0.7964)\tPrec 72.656% (71.751%)\n",
      "Epoch: [24][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.8066 (0.8006)\tPrec 65.625% (71.553%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.397 (0.397)\tLoss 0.7452 (0.7452)\tPrec 74.219% (74.219%)\n",
      " * Prec 69.070% \n",
      "best acc: 69.070000\n",
      "Epoch: [25][0/391]\tTime 0.741 (0.741)\tData 0.681 (0.681)\tLoss 0.7478 (0.7478)\tPrec 75.781% (75.781%)\n",
      "Epoch: [25][100/391]\tTime 0.047 (0.054)\tData 0.002 (0.009)\tLoss 0.7122 (0.8064)\tPrec 77.344% (71.334%)\n",
      "Epoch: [25][200/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.8164 (0.8058)\tPrec 71.094% (71.576%)\n",
      "Epoch: [25][300/391]\tTime 0.041 (0.048)\tData 0.002 (0.004)\tLoss 0.6896 (0.8016)\tPrec 78.125% (71.802%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 1.2264 (1.2264)\tPrec 60.156% (60.156%)\n",
      " * Prec 62.000% \n",
      "best acc: 69.070000\n",
      "Epoch: [26][0/391]\tTime 0.533 (0.533)\tData 0.432 (0.432)\tLoss 0.8462 (0.8462)\tPrec 75.000% (75.000%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.006)\tLoss 0.9876 (0.7983)\tPrec 65.625% (72.030%)\n",
      "Epoch: [26][200/391]\tTime 0.048 (0.047)\tData 0.003 (0.004)\tLoss 0.9401 (0.7908)\tPrec 64.844% (72.163%)\n",
      "Epoch: [26][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.003)\tLoss 0.8593 (0.7923)\tPrec 67.188% (72.106%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.420 (0.420)\tLoss 0.8733 (0.8733)\tPrec 71.094% (71.094%)\n",
      " * Prec 66.240% \n",
      "best acc: 69.070000\n",
      "Epoch: [27][0/391]\tTime 0.705 (0.705)\tData 0.642 (0.642)\tLoss 0.7689 (0.7689)\tPrec 70.312% (70.312%)\n",
      "Epoch: [27][100/391]\tTime 0.044 (0.053)\tData 0.002 (0.009)\tLoss 0.7035 (0.7827)\tPrec 74.219% (72.293%)\n",
      "Epoch: [27][200/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.7610 (0.7811)\tPrec 74.219% (72.349%)\n",
      "Epoch: [27][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.8408 (0.7776)\tPrec 67.969% (72.469%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.610 (0.610)\tLoss 0.8575 (0.8575)\tPrec 70.312% (70.312%)\n",
      " * Prec 67.330% \n",
      "best acc: 69.070000\n",
      "Epoch: [28][0/391]\tTime 0.589 (0.589)\tData 0.533 (0.533)\tLoss 0.6337 (0.6337)\tPrec 77.344% (77.344%)\n",
      "Epoch: [28][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.007)\tLoss 0.7479 (0.7762)\tPrec 71.875% (72.757%)\n",
      "Epoch: [28][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.005)\tLoss 0.7356 (0.7781)\tPrec 77.344% (72.847%)\n",
      "Epoch: [28][300/391]\tTime 0.045 (0.045)\tData 0.002 (0.004)\tLoss 0.8843 (0.7735)\tPrec 68.750% (72.812%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.424 (0.424)\tLoss 0.9640 (0.9640)\tPrec 66.406% (66.406%)\n",
      " * Prec 63.550% \n",
      "best acc: 69.070000\n",
      "Epoch: [29][0/391]\tTime 0.593 (0.593)\tData 0.535 (0.535)\tLoss 0.5894 (0.5894)\tPrec 80.469% (80.469%)\n",
      "Epoch: [29][100/391]\tTime 0.046 (0.052)\tData 0.002 (0.007)\tLoss 0.8643 (0.7550)\tPrec 69.531% (73.724%)\n",
      "Epoch: [29][200/391]\tTime 0.050 (0.050)\tData 0.003 (0.005)\tLoss 0.7433 (0.7660)\tPrec 76.562% (73.333%)\n",
      "Epoch: [29][300/391]\tTime 0.043 (0.049)\tData 0.002 (0.004)\tLoss 0.7213 (0.7688)\tPrec 71.094% (73.209%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.442 (0.442)\tLoss 0.7127 (0.7127)\tPrec 75.000% (75.000%)\n",
      " * Prec 68.940% \n",
      "best acc: 69.070000\n",
      "Epoch: [30][0/391]\tTime 0.584 (0.584)\tData 0.523 (0.523)\tLoss 0.8206 (0.8206)\tPrec 73.438% (73.438%)\n",
      "Epoch: [30][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.007)\tLoss 0.7801 (0.7609)\tPrec 76.562% (73.484%)\n",
      "Epoch: [30][200/391]\tTime 0.038 (0.048)\tData 0.002 (0.005)\tLoss 0.6297 (0.7678)\tPrec 78.125% (73.301%)\n",
      "Epoch: [30][300/391]\tTime 0.050 (0.048)\tData 0.002 (0.004)\tLoss 0.7781 (0.7649)\tPrec 74.219% (73.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.513 (0.513)\tLoss 0.8492 (0.8492)\tPrec 68.750% (68.750%)\n",
      " * Prec 68.010% \n",
      "best acc: 69.070000\n",
      "Epoch: [31][0/391]\tTime 0.830 (0.830)\tData 0.768 (0.768)\tLoss 0.6092 (0.6092)\tPrec 77.344% (77.344%)\n",
      "Epoch: [31][100/391]\tTime 0.039 (0.054)\tData 0.002 (0.010)\tLoss 0.9905 (0.7509)\tPrec 68.750% (73.523%)\n",
      "Epoch: [31][200/391]\tTime 0.041 (0.049)\tData 0.002 (0.006)\tLoss 0.8281 (0.7543)\tPrec 70.312% (73.484%)\n",
      "Epoch: [31][300/391]\tTime 0.040 (0.047)\tData 0.002 (0.005)\tLoss 0.6618 (0.7585)\tPrec 73.438% (73.313%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.405 (0.405)\tLoss 1.0043 (1.0043)\tPrec 66.406% (66.406%)\n",
      " * Prec 66.450% \n",
      "best acc: 69.070000\n",
      "Epoch: [32][0/391]\tTime 0.661 (0.661)\tData 0.569 (0.569)\tLoss 0.7275 (0.7275)\tPrec 78.906% (78.906%)\n",
      "Epoch: [32][100/391]\tTime 0.047 (0.052)\tData 0.002 (0.008)\tLoss 0.7775 (0.7542)\tPrec 71.875% (73.762%)\n",
      "Epoch: [32][200/391]\tTime 0.042 (0.048)\tData 0.002 (0.005)\tLoss 0.6211 (0.7568)\tPrec 81.250% (73.336%)\n",
      "Epoch: [32][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.7112 (0.7570)\tPrec 74.219% (73.500%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.403 (0.403)\tLoss 0.9301 (0.9301)\tPrec 64.844% (64.844%)\n",
      " * Prec 69.260% \n",
      "best acc: 69.260000\n",
      "Epoch: [33][0/391]\tTime 0.582 (0.582)\tData 0.512 (0.512)\tLoss 0.8997 (0.8997)\tPrec 69.531% (69.531%)\n",
      "Epoch: [33][100/391]\tTime 0.045 (0.051)\tData 0.002 (0.007)\tLoss 0.7579 (0.7508)\tPrec 71.875% (73.755%)\n",
      "Epoch: [33][200/391]\tTime 0.040 (0.050)\tData 0.002 (0.005)\tLoss 0.7883 (0.7457)\tPrec 72.656% (73.842%)\n",
      "Epoch: [33][300/391]\tTime 0.039 (0.050)\tData 0.002 (0.004)\tLoss 0.8267 (0.7428)\tPrec 71.094% (73.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.376 (0.376)\tLoss 0.9621 (0.9621)\tPrec 67.188% (67.188%)\n",
      " * Prec 64.120% \n",
      "best acc: 69.260000\n",
      "Epoch: [34][0/391]\tTime 0.848 (0.848)\tData 0.792 (0.792)\tLoss 0.8805 (0.8805)\tPrec 68.750% (68.750%)\n",
      "Epoch: [34][100/391]\tTime 0.042 (0.051)\tData 0.002 (0.010)\tLoss 0.6359 (0.7438)\tPrec 77.344% (73.793%)\n",
      "Epoch: [34][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.006)\tLoss 0.7719 (0.7374)\tPrec 76.562% (73.997%)\n",
      "Epoch: [34][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.005)\tLoss 0.8032 (0.7396)\tPrec 69.531% (73.993%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.419 (0.419)\tLoss 1.1614 (1.1614)\tPrec 62.500% (62.500%)\n",
      " * Prec 61.000% \n",
      "best acc: 69.260000\n",
      "Epoch: [35][0/391]\tTime 0.565 (0.565)\tData 0.507 (0.507)\tLoss 0.7958 (0.7958)\tPrec 71.875% (71.875%)\n",
      "Epoch: [35][100/391]\tTime 0.041 (0.048)\tData 0.002 (0.007)\tLoss 0.8786 (0.7369)\tPrec 72.656% (74.134%)\n",
      "Epoch: [35][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.005)\tLoss 0.6567 (0.7323)\tPrec 75.000% (73.982%)\n",
      "Epoch: [35][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.004)\tLoss 0.6877 (0.7333)\tPrec 75.781% (74.073%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.492 (0.492)\tLoss 0.8314 (0.8314)\tPrec 71.094% (71.094%)\n",
      " * Prec 69.400% \n",
      "best acc: 69.400000\n",
      "Epoch: [36][0/391]\tTime 0.854 (0.854)\tData 0.789 (0.789)\tLoss 0.6557 (0.6557)\tPrec 78.906% (78.906%)\n",
      "Epoch: [36][100/391]\tTime 0.048 (0.055)\tData 0.002 (0.010)\tLoss 1.0391 (0.7281)\tPrec 68.750% (74.675%)\n",
      "Epoch: [36][200/391]\tTime 0.044 (0.051)\tData 0.002 (0.006)\tLoss 0.7766 (0.7359)\tPrec 73.438% (74.401%)\n",
      "Epoch: [36][300/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.7872 (0.7326)\tPrec 73.438% (74.465%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.506 (0.506)\tLoss 0.8627 (0.8627)\tPrec 70.312% (70.312%)\n",
      " * Prec 67.230% \n",
      "best acc: 69.400000\n",
      "Epoch: [37][0/391]\tTime 1.016 (1.016)\tData 0.955 (0.955)\tLoss 0.6422 (0.6422)\tPrec 82.812% (82.812%)\n",
      "Epoch: [37][100/391]\tTime 0.053 (0.057)\tData 0.003 (0.012)\tLoss 0.6979 (0.7132)\tPrec 74.219% (75.008%)\n",
      "Epoch: [37][200/391]\tTime 0.042 (0.050)\tData 0.002 (0.007)\tLoss 0.6086 (0.7199)\tPrec 76.562% (74.565%)\n",
      "Epoch: [37][300/391]\tTime 0.053 (0.048)\tData 0.002 (0.005)\tLoss 0.7152 (0.7286)\tPrec 78.125% (74.291%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.9936 (0.9936)\tPrec 66.406% (66.406%)\n",
      " * Prec 65.260% \n",
      "best acc: 69.400000\n",
      "Epoch: [38][0/391]\tTime 0.621 (0.621)\tData 0.528 (0.528)\tLoss 0.6782 (0.6782)\tPrec 76.562% (76.562%)\n",
      "Epoch: [38][100/391]\tTime 0.047 (0.052)\tData 0.002 (0.007)\tLoss 0.7073 (0.7209)\tPrec 73.438% (74.938%)\n",
      "Epoch: [38][200/391]\tTime 0.049 (0.049)\tData 0.003 (0.005)\tLoss 0.7878 (0.7244)\tPrec 75.781% (74.681%)\n",
      "Epoch: [38][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.004)\tLoss 0.7024 (0.7263)\tPrec 75.781% (74.637%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.349 (0.349)\tLoss 0.9472 (0.9472)\tPrec 65.625% (65.625%)\n",
      " * Prec 65.280% \n",
      "best acc: 69.400000\n",
      "Epoch: [39][0/391]\tTime 0.903 (0.903)\tData 0.735 (0.735)\tLoss 0.6507 (0.6507)\tPrec 75.000% (75.000%)\n",
      "Epoch: [39][100/391]\tTime 0.047 (0.057)\tData 0.002 (0.009)\tLoss 0.7031 (0.7217)\tPrec 76.562% (74.667%)\n",
      "Epoch: [39][200/391]\tTime 0.051 (0.053)\tData 0.002 (0.006)\tLoss 0.9211 (0.7152)\tPrec 68.750% (74.969%)\n",
      "Epoch: [39][300/391]\tTime 0.049 (0.052)\tData 0.002 (0.004)\tLoss 0.6693 (0.7205)\tPrec 75.000% (74.652%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.460 (0.460)\tLoss 1.0180 (1.0180)\tPrec 62.500% (62.500%)\n",
      " * Prec 63.320% \n",
      "best acc: 69.400000\n",
      "Epoch: [40][0/391]\tTime 0.777 (0.777)\tData 0.675 (0.675)\tLoss 0.8005 (0.8005)\tPrec 64.062% (64.062%)\n",
      "Epoch: [40][100/391]\tTime 0.050 (0.056)\tData 0.002 (0.009)\tLoss 0.7384 (0.7170)\tPrec 74.219% (75.131%)\n",
      "Epoch: [40][200/391]\tTime 0.051 (0.052)\tData 0.002 (0.006)\tLoss 0.7451 (0.7032)\tPrec 76.562% (75.377%)\n",
      "Epoch: [40][300/391]\tTime 0.042 (0.050)\tData 0.002 (0.004)\tLoss 0.7664 (0.7091)\tPrec 73.438% (75.236%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.548 (0.548)\tLoss 0.8765 (0.8765)\tPrec 73.438% (73.438%)\n",
      " * Prec 68.920% \n",
      "best acc: 69.400000\n",
      "Epoch: [41][0/391]\tTime 0.501 (0.501)\tData 0.408 (0.408)\tLoss 0.6271 (0.6271)\tPrec 75.000% (75.000%)\n",
      "Epoch: [41][100/391]\tTime 0.039 (0.051)\tData 0.002 (0.006)\tLoss 0.5844 (0.7094)\tPrec 78.906% (75.688%)\n",
      "Epoch: [41][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.7168 (0.7096)\tPrec 71.094% (75.354%)\n",
      "Epoch: [41][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.8186 (0.7078)\tPrec 75.781% (75.304%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.378 (0.378)\tLoss 1.0691 (1.0691)\tPrec 64.844% (64.844%)\n",
      " * Prec 63.620% \n",
      "best acc: 69.400000\n",
      "Epoch: [42][0/391]\tTime 0.491 (0.491)\tData 0.431 (0.431)\tLoss 0.7754 (0.7754)\tPrec 70.312% (70.312%)\n",
      "Epoch: [42][100/391]\tTime 0.040 (0.051)\tData 0.002 (0.006)\tLoss 0.6821 (0.7009)\tPrec 75.781% (75.557%)\n",
      "Epoch: [42][200/391]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.6690 (0.7039)\tPrec 73.438% (75.136%)\n",
      "Epoch: [42][300/391]\tTime 0.052 (0.049)\tData 0.002 (0.004)\tLoss 0.6572 (0.6996)\tPrec 75.000% (75.296%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.362 (0.362)\tLoss 0.8593 (0.8593)\tPrec 69.531% (69.531%)\n",
      " * Prec 67.910% \n",
      "best acc: 69.400000\n",
      "Epoch: [43][0/391]\tTime 0.703 (0.703)\tData 0.641 (0.641)\tLoss 0.6766 (0.6766)\tPrec 79.688% (79.688%)\n",
      "Epoch: [43][100/391]\tTime 0.044 (0.051)\tData 0.002 (0.008)\tLoss 0.8255 (0.7129)\tPrec 70.312% (75.062%)\n",
      "Epoch: [43][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.005)\tLoss 0.8643 (0.7026)\tPrec 72.656% (75.412%)\n",
      "Epoch: [43][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.8528 (0.7031)\tPrec 74.219% (75.459%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.285 (0.285)\tLoss 0.8445 (0.8445)\tPrec 66.406% (66.406%)\n",
      " * Prec 70.380% \n",
      "best acc: 70.380000\n",
      "Epoch: [44][0/391]\tTime 0.485 (0.485)\tData 0.425 (0.425)\tLoss 0.8751 (0.8751)\tPrec 69.531% (69.531%)\n",
      "Epoch: [44][100/391]\tTime 0.049 (0.050)\tData 0.003 (0.006)\tLoss 0.6517 (0.6882)\tPrec 71.875% (76.021%)\n",
      "Epoch: [44][200/391]\tTime 0.050 (0.047)\tData 0.002 (0.004)\tLoss 0.6612 (0.6931)\tPrec 77.344% (75.684%)\n",
      "Epoch: [44][300/391]\tTime 0.042 (0.046)\tData 0.002 (0.003)\tLoss 0.6019 (0.6956)\tPrec 80.469% (75.568%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.326 (0.326)\tLoss 0.7308 (0.7308)\tPrec 74.219% (74.219%)\n",
      " * Prec 69.100% \n",
      "best acc: 70.380000\n",
      "Epoch: [45][0/391]\tTime 0.617 (0.617)\tData 0.561 (0.561)\tLoss 0.5956 (0.5956)\tPrec 76.562% (76.562%)\n",
      "Epoch: [45][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.007)\tLoss 0.6758 (0.6887)\tPrec 75.000% (75.851%)\n",
      "Epoch: [45][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.005)\tLoss 0.7748 (0.6976)\tPrec 76.562% (75.684%)\n",
      "Epoch: [45][300/391]\tTime 0.047 (0.046)\tData 0.002 (0.004)\tLoss 0.6657 (0.6910)\tPrec 77.344% (75.984%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.449 (0.449)\tLoss 0.9609 (0.9609)\tPrec 70.312% (70.312%)\n",
      " * Prec 68.190% \n",
      "best acc: 70.380000\n",
      "Epoch: [46][0/391]\tTime 0.476 (0.476)\tData 0.420 (0.420)\tLoss 0.6902 (0.6902)\tPrec 73.438% (73.438%)\n",
      "Epoch: [46][100/391]\tTime 0.048 (0.050)\tData 0.002 (0.006)\tLoss 0.8496 (0.6932)\tPrec 66.406% (75.650%)\n",
      "Epoch: [46][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.5859 (0.6980)\tPrec 79.688% (75.595%)\n",
      "Epoch: [46][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.004)\tLoss 0.6553 (0.6928)\tPrec 77.344% (75.753%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.800 (0.800)\tLoss 1.0906 (1.0906)\tPrec 66.406% (66.406%)\n",
      " * Prec 64.090% \n",
      "best acc: 70.380000\n",
      "Epoch: [47][0/391]\tTime 0.673 (0.673)\tData 0.583 (0.583)\tLoss 0.7786 (0.7786)\tPrec 73.438% (73.438%)\n",
      "Epoch: [47][100/391]\tTime 0.038 (0.052)\tData 0.002 (0.008)\tLoss 0.7783 (0.6757)\tPrec 72.656% (76.191%)\n",
      "Epoch: [47][200/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.7259 (0.6837)\tPrec 71.875% (76.116%)\n",
      "Epoch: [47][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.7454 (0.6880)\tPrec 75.781% (75.846%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 0.8500 (0.8500)\tPrec 67.188% (67.188%)\n",
      " * Prec 67.420% \n",
      "best acc: 70.380000\n",
      "Epoch: [48][0/391]\tTime 0.647 (0.647)\tData 0.589 (0.589)\tLoss 0.8315 (0.8315)\tPrec 72.656% (72.656%)\n",
      "Epoch: [48][100/391]\tTime 0.046 (0.051)\tData 0.002 (0.008)\tLoss 0.6742 (0.6623)\tPrec 74.219% (76.663%)\n",
      "Epoch: [48][200/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.7200 (0.6693)\tPrec 73.438% (76.566%)\n",
      "Epoch: [48][300/391]\tTime 0.050 (0.048)\tData 0.002 (0.004)\tLoss 0.8335 (0.6747)\tPrec 67.188% (76.441%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.7944 (0.7944)\tPrec 71.875% (71.875%)\n",
      " * Prec 69.660% \n",
      "best acc: 70.380000\n",
      "Epoch: [49][0/391]\tTime 0.596 (0.596)\tData 0.534 (0.534)\tLoss 0.6363 (0.6363)\tPrec 78.906% (78.906%)\n",
      "Epoch: [49][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.007)\tLoss 0.6185 (0.6895)\tPrec 78.906% (75.719%)\n",
      "Epoch: [49][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.005)\tLoss 0.7374 (0.6761)\tPrec 76.562% (76.399%)\n",
      "Epoch: [49][300/391]\tTime 0.048 (0.045)\tData 0.003 (0.004)\tLoss 0.7183 (0.6764)\tPrec 71.875% (76.482%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.430 (0.430)\tLoss 0.8922 (0.8922)\tPrec 72.656% (72.656%)\n",
      " * Prec 68.810% \n",
      "best acc: 70.380000\n",
      "Epoch: [50][0/391]\tTime 0.460 (0.460)\tData 0.411 (0.411)\tLoss 0.5312 (0.5312)\tPrec 79.688% (79.688%)\n",
      "Epoch: [50][100/391]\tTime 0.039 (0.049)\tData 0.002 (0.006)\tLoss 0.6501 (0.6519)\tPrec 78.906% (77.034%)\n",
      "Epoch: [50][200/391]\tTime 0.041 (0.046)\tData 0.002 (0.004)\tLoss 0.6908 (0.6673)\tPrec 71.875% (76.481%)\n",
      "Epoch: [50][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.7979 (0.6701)\tPrec 67.188% (76.472%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.342 (0.342)\tLoss 0.8984 (0.8984)\tPrec 72.656% (72.656%)\n",
      " * Prec 69.070% \n",
      "best acc: 70.380000\n",
      "Epoch: [51][0/391]\tTime 0.511 (0.511)\tData 0.455 (0.455)\tLoss 0.6708 (0.6708)\tPrec 75.781% (75.781%)\n",
      "Epoch: [51][100/391]\tTime 0.039 (0.048)\tData 0.002 (0.006)\tLoss 0.6617 (0.6642)\tPrec 82.031% (76.949%)\n",
      "Epoch: [51][200/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.6748 (0.6662)\tPrec 75.000% (76.920%)\n",
      "Epoch: [51][300/391]\tTime 0.038 (0.045)\tData 0.002 (0.003)\tLoss 0.5939 (0.6700)\tPrec 78.906% (76.783%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.509 (0.509)\tLoss 0.8510 (0.8510)\tPrec 74.219% (74.219%)\n",
      " * Prec 71.370% \n",
      "best acc: 71.370000\n",
      "Epoch: [52][0/391]\tTime 0.625 (0.625)\tData 0.573 (0.573)\tLoss 0.6149 (0.6149)\tPrec 81.250% (81.250%)\n",
      "Epoch: [52][100/391]\tTime 0.050 (0.055)\tData 0.002 (0.008)\tLoss 0.7192 (0.6526)\tPrec 71.094% (77.096%)\n",
      "Epoch: [52][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.8267 (0.6650)\tPrec 73.438% (76.664%)\n",
      "Epoch: [52][300/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.7298 (0.6686)\tPrec 74.219% (76.664%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.507 (0.507)\tLoss 0.7388 (0.7388)\tPrec 73.438% (73.438%)\n",
      " * Prec 73.200% \n",
      "best acc: 73.200000\n",
      "Epoch: [53][0/391]\tTime 0.987 (0.987)\tData 0.903 (0.903)\tLoss 0.6669 (0.6669)\tPrec 78.906% (78.906%)\n",
      "Epoch: [53][100/391]\tTime 0.053 (0.060)\tData 0.002 (0.011)\tLoss 0.7418 (0.6682)\tPrec 76.562% (76.493%)\n",
      "Epoch: [53][200/391]\tTime 0.049 (0.053)\tData 0.002 (0.007)\tLoss 0.8363 (0.6667)\tPrec 69.531% (76.578%)\n",
      "Epoch: [53][300/391]\tTime 0.037 (0.050)\tData 0.002 (0.005)\tLoss 0.7388 (0.6627)\tPrec 75.000% (76.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.356 (0.356)\tLoss 0.7905 (0.7905)\tPrec 75.781% (75.781%)\n",
      " * Prec 69.560% \n",
      "best acc: 73.200000\n",
      "Epoch: [54][0/391]\tTime 0.477 (0.477)\tData 0.418 (0.418)\tLoss 0.6623 (0.6623)\tPrec 73.438% (73.438%)\n",
      "Epoch: [54][100/391]\tTime 0.041 (0.050)\tData 0.002 (0.006)\tLoss 0.5416 (0.6442)\tPrec 79.688% (77.491%)\n",
      "Epoch: [54][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.004)\tLoss 0.4762 (0.6542)\tPrec 82.031% (76.951%)\n",
      "Epoch: [54][300/391]\tTime 0.037 (0.045)\tData 0.002 (0.003)\tLoss 0.5580 (0.6580)\tPrec 79.688% (76.814%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.571 (0.571)\tLoss 0.7860 (0.7860)\tPrec 73.438% (73.438%)\n",
      " * Prec 71.980% \n",
      "best acc: 73.200000\n",
      "Epoch: [55][0/391]\tTime 0.611 (0.611)\tData 0.514 (0.514)\tLoss 0.5672 (0.5672)\tPrec 80.469% (80.469%)\n",
      "Epoch: [55][100/391]\tTime 0.043 (0.052)\tData 0.002 (0.007)\tLoss 0.7219 (0.6383)\tPrec 75.781% (77.924%)\n",
      "Epoch: [55][200/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.6963 (0.6523)\tPrec 75.781% (77.433%)\n",
      "Epoch: [55][300/391]\tTime 0.040 (0.047)\tData 0.002 (0.004)\tLoss 0.5858 (0.6555)\tPrec 82.031% (77.193%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.425 (0.425)\tLoss 0.6164 (0.6164)\tPrec 78.125% (78.125%)\n",
      " * Prec 71.610% \n",
      "best acc: 73.200000\n",
      "Epoch: [56][0/391]\tTime 0.595 (0.595)\tData 0.537 (0.537)\tLoss 0.5925 (0.5925)\tPrec 75.000% (75.000%)\n",
      "Epoch: [56][100/391]\tTime 0.046 (0.052)\tData 0.002 (0.007)\tLoss 0.8376 (0.6506)\tPrec 71.094% (77.382%)\n",
      "Epoch: [56][200/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.8210 (0.6504)\tPrec 72.656% (77.317%)\n",
      "Epoch: [56][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.5401 (0.6523)\tPrec 79.688% (77.191%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.994 (0.994)\tLoss 0.7197 (0.7197)\tPrec 75.781% (75.781%)\n",
      " * Prec 72.980% \n",
      "best acc: 73.200000\n",
      "Epoch: [57][0/391]\tTime 0.613 (0.613)\tData 0.551 (0.551)\tLoss 0.7649 (0.7649)\tPrec 75.781% (75.781%)\n",
      "Epoch: [57][100/391]\tTime 0.041 (0.052)\tData 0.002 (0.008)\tLoss 0.6393 (0.6478)\tPrec 77.344% (77.599%)\n",
      "Epoch: [57][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.005)\tLoss 0.6880 (0.6501)\tPrec 79.688% (77.441%)\n",
      "Epoch: [57][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.6735 (0.6489)\tPrec 76.562% (77.414%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.595 (0.595)\tLoss 0.7790 (0.7790)\tPrec 75.000% (75.000%)\n",
      " * Prec 71.510% \n",
      "best acc: 73.200000\n",
      "Epoch: [58][0/391]\tTime 0.589 (0.589)\tData 0.535 (0.535)\tLoss 0.6233 (0.6233)\tPrec 80.469% (80.469%)\n",
      "Epoch: [58][100/391]\tTime 0.040 (0.050)\tData 0.002 (0.007)\tLoss 0.6921 (0.6524)\tPrec 78.906% (77.328%)\n",
      "Epoch: [58][200/391]\tTime 0.033 (0.047)\tData 0.002 (0.005)\tLoss 0.5854 (0.6401)\tPrec 79.688% (77.799%)\n",
      "Epoch: [58][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.6100 (0.6439)\tPrec 78.125% (77.608%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.9137 (0.9137)\tPrec 65.625% (65.625%)\n",
      " * Prec 66.360% \n",
      "best acc: 73.200000\n",
      "Epoch: [59][0/391]\tTime 0.599 (0.599)\tData 0.539 (0.539)\tLoss 0.6217 (0.6217)\tPrec 72.656% (72.656%)\n",
      "Epoch: [59][100/391]\tTime 0.047 (0.053)\tData 0.002 (0.008)\tLoss 0.5021 (0.6465)\tPrec 79.688% (77.367%)\n",
      "Epoch: [59][200/391]\tTime 0.041 (0.050)\tData 0.002 (0.005)\tLoss 0.6906 (0.6482)\tPrec 79.688% (77.375%)\n",
      "Epoch: [59][300/391]\tTime 0.051 (0.049)\tData 0.002 (0.004)\tLoss 0.7883 (0.6548)\tPrec 70.312% (77.115%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.7950 (0.7950)\tPrec 70.312% (70.312%)\n",
      " * Prec 69.340% \n",
      "best acc: 73.200000\n",
      "Epoch: [60][0/391]\tTime 0.412 (0.412)\tData 0.328 (0.328)\tLoss 0.6670 (0.6670)\tPrec 78.906% (78.906%)\n",
      "Epoch: [60][100/391]\tTime 0.051 (0.050)\tData 0.002 (0.005)\tLoss 0.7386 (0.6481)\tPrec 76.562% (77.065%)\n",
      "Epoch: [60][200/391]\tTime 0.044 (0.049)\tData 0.002 (0.003)\tLoss 0.5458 (0.6522)\tPrec 79.688% (77.044%)\n",
      "Epoch: [60][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.6032 (0.6591)\tPrec 77.344% (76.905%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.7621 (0.7621)\tPrec 74.219% (74.219%)\n",
      " * Prec 70.000% \n",
      "best acc: 73.200000\n",
      "Epoch: [61][0/391]\tTime 0.612 (0.612)\tData 0.545 (0.545)\tLoss 0.6162 (0.6162)\tPrec 76.562% (76.562%)\n",
      "Epoch: [61][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.007)\tLoss 0.8785 (0.6457)\tPrec 64.844% (77.135%)\n",
      "Epoch: [61][200/391]\tTime 0.053 (0.047)\tData 0.002 (0.005)\tLoss 0.7604 (0.6430)\tPrec 75.000% (77.258%)\n",
      "Epoch: [61][300/391]\tTime 0.050 (0.047)\tData 0.003 (0.004)\tLoss 0.5136 (0.6518)\tPrec 84.375% (77.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.318 (0.318)\tLoss 1.0142 (1.0142)\tPrec 72.656% (72.656%)\n",
      " * Prec 67.130% \n",
      "best acc: 73.200000\n",
      "Epoch: [62][0/391]\tTime 0.578 (0.578)\tData 0.520 (0.520)\tLoss 0.5700 (0.5700)\tPrec 82.812% (82.812%)\n",
      "Epoch: [62][100/391]\tTime 0.044 (0.052)\tData 0.002 (0.007)\tLoss 0.6767 (0.6612)\tPrec 78.125% (76.617%)\n",
      "Epoch: [62][200/391]\tTime 0.038 (0.048)\tData 0.002 (0.005)\tLoss 0.6351 (0.6472)\tPrec 76.562% (77.212%)\n",
      "Epoch: [62][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.7158 (0.6448)\tPrec 76.562% (77.445%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.303 (0.303)\tLoss 0.9636 (0.9636)\tPrec 72.656% (72.656%)\n",
      " * Prec 65.730% \n",
      "best acc: 73.200000\n",
      "Epoch: [63][0/391]\tTime 0.520 (0.520)\tData 0.460 (0.460)\tLoss 0.6836 (0.6836)\tPrec 75.781% (75.781%)\n",
      "Epoch: [63][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.6146 (0.6378)\tPrec 81.250% (77.382%)\n",
      "Epoch: [63][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.7688 (0.6453)\tPrec 76.562% (77.196%)\n",
      "Epoch: [63][300/391]\tTime 0.041 (0.047)\tData 0.002 (0.003)\tLoss 0.5651 (0.6425)\tPrec 83.594% (77.411%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.408 (0.408)\tLoss 0.8589 (0.8589)\tPrec 72.656% (72.656%)\n",
      " * Prec 70.000% \n",
      "best acc: 73.200000\n",
      "Epoch: [64][0/391]\tTime 0.581 (0.581)\tData 0.529 (0.529)\tLoss 0.5834 (0.5834)\tPrec 77.344% (77.344%)\n",
      "Epoch: [64][100/391]\tTime 0.037 (0.050)\tData 0.002 (0.007)\tLoss 0.6428 (0.6514)\tPrec 75.781% (77.042%)\n",
      "Epoch: [64][200/391]\tTime 0.050 (0.047)\tData 0.002 (0.005)\tLoss 0.8134 (0.6454)\tPrec 75.781% (77.317%)\n",
      "Epoch: [64][300/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.6496 (0.6394)\tPrec 80.469% (77.583%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.459 (0.459)\tLoss 0.6482 (0.6482)\tPrec 77.344% (77.344%)\n",
      " * Prec 74.060% \n",
      "best acc: 74.060000\n",
      "Epoch: [65][0/391]\tTime 0.637 (0.637)\tData 0.562 (0.562)\tLoss 0.5563 (0.5563)\tPrec 82.031% (82.031%)\n",
      "Epoch: [65][100/391]\tTime 0.038 (0.051)\tData 0.002 (0.008)\tLoss 0.6002 (0.6364)\tPrec 79.688% (77.607%)\n",
      "Epoch: [65][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.005)\tLoss 0.7990 (0.6492)\tPrec 74.219% (77.192%)\n",
      "Epoch: [65][300/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.5200 (0.6403)\tPrec 84.375% (77.580%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.407 (0.407)\tLoss 0.9327 (0.9327)\tPrec 72.656% (72.656%)\n",
      " * Prec 68.430% \n",
      "best acc: 74.060000\n",
      "Epoch: [66][0/391]\tTime 0.578 (0.578)\tData 0.515 (0.515)\tLoss 0.7026 (0.7026)\tPrec 75.000% (75.000%)\n",
      "Epoch: [66][100/391]\tTime 0.048 (0.054)\tData 0.002 (0.007)\tLoss 0.6436 (0.6379)\tPrec 77.344% (77.715%)\n",
      "Epoch: [66][200/391]\tTime 0.044 (0.050)\tData 0.002 (0.005)\tLoss 0.6223 (0.6385)\tPrec 82.812% (77.662%)\n",
      "Epoch: [66][300/391]\tTime 0.054 (0.048)\tData 0.003 (0.004)\tLoss 0.7887 (0.6375)\tPrec 74.219% (77.684%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.422 (0.422)\tLoss 0.9540 (0.9540)\tPrec 69.531% (69.531%)\n",
      " * Prec 70.780% \n",
      "best acc: 74.060000\n",
      "Epoch: [67][0/391]\tTime 0.686 (0.686)\tData 0.631 (0.631)\tLoss 0.5546 (0.5546)\tPrec 82.031% (82.031%)\n",
      "Epoch: [67][100/391]\tTime 0.038 (0.055)\tData 0.002 (0.009)\tLoss 0.6593 (0.6248)\tPrec 76.562% (78.164%)\n",
      "Epoch: [67][200/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.5954 (0.6303)\tPrec 81.250% (77.942%)\n",
      "Epoch: [67][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5504 (0.6364)\tPrec 79.688% (77.707%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.671 (0.671)\tLoss 0.6703 (0.6703)\tPrec 78.125% (78.125%)\n",
      " * Prec 74.750% \n",
      "best acc: 74.750000\n",
      "Epoch: [68][0/391]\tTime 0.681 (0.681)\tData 0.621 (0.621)\tLoss 0.5637 (0.5637)\tPrec 78.125% (78.125%)\n",
      "Epoch: [68][100/391]\tTime 0.047 (0.053)\tData 0.002 (0.008)\tLoss 0.6747 (0.6287)\tPrec 75.781% (78.380%)\n",
      "Epoch: [68][200/391]\tTime 0.037 (0.050)\tData 0.002 (0.005)\tLoss 0.6406 (0.6278)\tPrec 78.125% (78.176%)\n",
      "Epoch: [68][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.8438 (0.6323)\tPrec 73.438% (77.993%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.458 (0.458)\tLoss 0.7946 (0.7946)\tPrec 72.656% (72.656%)\n",
      " * Prec 72.620% \n",
      "best acc: 74.750000\n",
      "Epoch: [69][0/391]\tTime 1.081 (1.081)\tData 1.033 (1.033)\tLoss 0.7083 (0.7083)\tPrec 75.000% (75.000%)\n",
      "Epoch: [69][100/391]\tTime 0.049 (0.057)\tData 0.003 (0.012)\tLoss 0.5793 (0.6327)\tPrec 78.906% (78.140%)\n",
      "Epoch: [69][200/391]\tTime 0.043 (0.051)\tData 0.002 (0.007)\tLoss 0.7859 (0.6352)\tPrec 66.406% (77.946%)\n",
      "Epoch: [69][300/391]\tTime 0.043 (0.049)\tData 0.002 (0.006)\tLoss 0.5333 (0.6380)\tPrec 81.250% (77.725%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.792 (0.792)\tLoss 0.8272 (0.8272)\tPrec 73.438% (73.438%)\n",
      " * Prec 70.820% \n",
      "best acc: 74.750000\n",
      "Epoch: [70][0/391]\tTime 0.564 (0.564)\tData 0.508 (0.508)\tLoss 0.6028 (0.6028)\tPrec 77.344% (77.344%)\n",
      "Epoch: [70][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.007)\tLoss 0.4903 (0.6212)\tPrec 82.031% (78.612%)\n",
      "Epoch: [70][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.005)\tLoss 0.5855 (0.6219)\tPrec 82.812% (78.284%)\n",
      "Epoch: [70][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.004)\tLoss 0.5819 (0.6276)\tPrec 78.125% (78.229%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.8282 (0.8282)\tPrec 69.531% (69.531%)\n",
      " * Prec 72.510% \n",
      "best acc: 74.750000\n",
      "Epoch: [71][0/391]\tTime 0.765 (0.765)\tData 0.709 (0.709)\tLoss 0.5621 (0.5621)\tPrec 79.688% (79.688%)\n",
      "Epoch: [71][100/391]\tTime 0.049 (0.054)\tData 0.002 (0.009)\tLoss 0.7481 (0.6174)\tPrec 73.438% (78.218%)\n",
      "Epoch: [71][200/391]\tTime 0.048 (0.050)\tData 0.002 (0.006)\tLoss 0.6094 (0.6180)\tPrec 76.562% (78.226%)\n",
      "Epoch: [71][300/391]\tTime 0.049 (0.049)\tData 0.002 (0.004)\tLoss 0.7117 (0.6252)\tPrec 72.656% (77.943%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.543 (0.543)\tLoss 0.7608 (0.7608)\tPrec 75.781% (75.781%)\n",
      " * Prec 73.200% \n",
      "best acc: 74.750000\n",
      "Epoch: [72][0/391]\tTime 0.491 (0.491)\tData 0.411 (0.411)\tLoss 0.7797 (0.7797)\tPrec 70.312% (70.312%)\n",
      "Epoch: [72][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.006)\tLoss 0.7136 (0.6240)\tPrec 73.438% (78.048%)\n",
      "Epoch: [72][200/391]\tTime 0.049 (0.047)\tData 0.002 (0.004)\tLoss 0.7594 (0.6215)\tPrec 71.094% (78.195%)\n",
      "Epoch: [72][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.6851 (0.6267)\tPrec 78.125% (78.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.408 (0.408)\tLoss 0.7097 (0.7097)\tPrec 75.781% (75.781%)\n",
      " * Prec 74.060% \n",
      "best acc: 74.750000\n",
      "Epoch: [73][0/391]\tTime 0.548 (0.548)\tData 0.493 (0.493)\tLoss 0.6762 (0.6762)\tPrec 76.562% (76.562%)\n",
      "Epoch: [73][100/391]\tTime 0.048 (0.048)\tData 0.002 (0.007)\tLoss 0.6125 (0.6241)\tPrec 76.562% (77.947%)\n",
      "Epoch: [73][200/391]\tTime 0.052 (0.047)\tData 0.002 (0.004)\tLoss 0.7328 (0.6290)\tPrec 71.094% (77.935%)\n",
      "Epoch: [73][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.004)\tLoss 0.6565 (0.6286)\tPrec 77.344% (77.933%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.340 (0.340)\tLoss 0.7391 (0.7391)\tPrec 71.094% (71.094%)\n",
      " * Prec 72.460% \n",
      "best acc: 74.750000\n",
      "Epoch: [74][0/391]\tTime 0.467 (0.467)\tData 0.408 (0.408)\tLoss 0.5417 (0.5417)\tPrec 81.250% (81.250%)\n",
      "Epoch: [74][100/391]\tTime 0.039 (0.048)\tData 0.002 (0.006)\tLoss 0.5576 (0.6287)\tPrec 83.594% (78.249%)\n",
      "Epoch: [74][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.004)\tLoss 0.5897 (0.6256)\tPrec 77.344% (78.109%)\n",
      "Epoch: [74][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.5921 (0.6272)\tPrec 81.250% (78.063%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.492 (0.492)\tLoss 0.9202 (0.9202)\tPrec 72.656% (72.656%)\n",
      " * Prec 71.640% \n",
      "best acc: 74.750000\n",
      "Epoch: [75][0/391]\tTime 0.659 (0.659)\tData 0.597 (0.597)\tLoss 0.6477 (0.6477)\tPrec 78.906% (78.906%)\n",
      "Epoch: [75][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.008)\tLoss 0.5863 (0.6424)\tPrec 81.250% (77.452%)\n",
      "Epoch: [75][200/391]\tTime 0.060 (0.048)\tData 0.002 (0.005)\tLoss 0.6472 (0.6351)\tPrec 78.125% (77.954%)\n",
      "Epoch: [75][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.6220 (0.6344)\tPrec 79.688% (77.993%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.380 (0.380)\tLoss 0.9280 (0.9280)\tPrec 75.000% (75.000%)\n",
      " * Prec 67.180% \n",
      "best acc: 74.750000\n",
      "Epoch: [76][0/391]\tTime 0.567 (0.567)\tData 0.508 (0.508)\tLoss 0.7862 (0.7862)\tPrec 71.094% (71.094%)\n",
      "Epoch: [76][100/391]\tTime 0.047 (0.052)\tData 0.002 (0.007)\tLoss 0.7891 (0.6383)\tPrec 71.875% (77.622%)\n",
      "Epoch: [76][200/391]\tTime 0.061 (0.049)\tData 0.003 (0.005)\tLoss 0.7756 (0.6282)\tPrec 74.219% (78.164%)\n",
      "Epoch: [76][300/391]\tTime 0.041 (0.048)\tData 0.002 (0.004)\tLoss 0.7047 (0.6264)\tPrec 72.656% (78.130%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.406 (0.406)\tLoss 0.7721 (0.7721)\tPrec 70.312% (70.312%)\n",
      " * Prec 72.470% \n",
      "best acc: 74.750000\n",
      "Epoch: [77][0/391]\tTime 0.582 (0.582)\tData 0.528 (0.528)\tLoss 0.6311 (0.6311)\tPrec 77.344% (77.344%)\n",
      "Epoch: [77][100/391]\tTime 0.044 (0.051)\tData 0.002 (0.007)\tLoss 0.6563 (0.6230)\tPrec 78.125% (78.202%)\n",
      "Epoch: [77][200/391]\tTime 0.049 (0.048)\tData 0.003 (0.005)\tLoss 0.6041 (0.6320)\tPrec 78.125% (77.997%)\n",
      "Epoch: [77][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5451 (0.6274)\tPrec 85.156% (78.096%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.361 (0.361)\tLoss 0.7836 (0.7836)\tPrec 73.438% (73.438%)\n",
      " * Prec 69.570% \n",
      "best acc: 74.750000\n",
      "Epoch: [78][0/391]\tTime 0.638 (0.638)\tData 0.581 (0.581)\tLoss 0.4813 (0.4813)\tPrec 82.031% (82.031%)\n",
      "Epoch: [78][100/391]\tTime 0.044 (0.054)\tData 0.002 (0.008)\tLoss 0.5436 (0.6273)\tPrec 77.344% (77.823%)\n",
      "Epoch: [78][200/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.6062 (0.6188)\tPrec 78.125% (78.331%)\n",
      "Epoch: [78][300/391]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.5544 (0.6190)\tPrec 81.250% (78.353%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.657 (0.657)\tLoss 0.7491 (0.7491)\tPrec 73.438% (73.438%)\n",
      " * Prec 73.460% \n",
      "best acc: 74.750000\n",
      "Epoch: [79][0/391]\tTime 0.576 (0.576)\tData 0.517 (0.517)\tLoss 0.6895 (0.6895)\tPrec 77.344% (77.344%)\n",
      "Epoch: [79][100/391]\tTime 0.043 (0.052)\tData 0.002 (0.007)\tLoss 0.6314 (0.6306)\tPrec 73.438% (77.970%)\n",
      "Epoch: [79][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.005)\tLoss 0.6513 (0.6202)\tPrec 77.344% (78.319%)\n",
      "Epoch: [79][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.004)\tLoss 0.5996 (0.6169)\tPrec 79.688% (78.429%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.433 (0.433)\tLoss 0.7166 (0.7166)\tPrec 78.125% (78.125%)\n",
      " * Prec 74.080% \n",
      "best acc: 74.750000\n",
      "Epoch: [80][0/391]\tTime 0.605 (0.605)\tData 0.541 (0.541)\tLoss 0.7775 (0.7775)\tPrec 71.094% (71.094%)\n",
      "Epoch: [80][100/391]\tTime 0.042 (0.054)\tData 0.002 (0.008)\tLoss 0.3980 (0.5629)\tPrec 87.500% (80.330%)\n",
      "Epoch: [80][200/391]\tTime 0.051 (0.051)\tData 0.003 (0.005)\tLoss 0.5286 (0.5530)\tPrec 75.781% (80.581%)\n",
      "Epoch: [80][300/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.4357 (0.5447)\tPrec 88.281% (80.923%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.480 (0.480)\tLoss 0.4934 (0.4934)\tPrec 83.594% (83.594%)\n",
      " * Prec 79.010% \n",
      "best acc: 79.010000\n",
      "Epoch: [81][0/391]\tTime 0.744 (0.744)\tData 0.705 (0.705)\tLoss 0.4884 (0.4884)\tPrec 84.375% (84.375%)\n",
      "Epoch: [81][100/391]\tTime 0.052 (0.056)\tData 0.003 (0.009)\tLoss 0.4895 (0.5170)\tPrec 82.031% (81.900%)\n",
      "Epoch: [81][200/391]\tTime 0.043 (0.051)\tData 0.002 (0.006)\tLoss 0.4455 (0.5145)\tPrec 83.594% (81.981%)\n",
      "Epoch: [81][300/391]\tTime 0.037 (0.049)\tData 0.002 (0.005)\tLoss 0.4979 (0.5225)\tPrec 82.031% (81.663%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.450 (0.450)\tLoss 0.6086 (0.6086)\tPrec 81.250% (81.250%)\n",
      " * Prec 77.400% \n",
      "best acc: 79.010000\n",
      "Epoch: [82][0/391]\tTime 0.722 (0.722)\tData 0.660 (0.660)\tLoss 0.4648 (0.4648)\tPrec 83.594% (83.594%)\n",
      "Epoch: [82][100/391]\tTime 0.043 (0.053)\tData 0.002 (0.009)\tLoss 0.6489 (0.5172)\tPrec 81.250% (82.124%)\n",
      "Epoch: [82][200/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.5837 (0.5189)\tPrec 82.031% (81.872%)\n",
      "Epoch: [82][300/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.5903 (0.5216)\tPrec 82.031% (81.839%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.432 (0.432)\tLoss 0.5419 (0.5419)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.620% \n",
      "best acc: 79.010000\n",
      "Epoch: [83][0/391]\tTime 0.608 (0.608)\tData 0.544 (0.544)\tLoss 0.3584 (0.3584)\tPrec 87.500% (87.500%)\n",
      "Epoch: [83][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.007)\tLoss 0.5288 (0.5021)\tPrec 80.469% (82.503%)\n",
      "Epoch: [83][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.005)\tLoss 0.5638 (0.5118)\tPrec 79.688% (82.249%)\n",
      "Epoch: [83][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.4567 (0.5150)\tPrec 85.156% (82.130%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.5547 (0.5547)\tPrec 76.562% (76.562%)\n",
      " * Prec 79.100% \n",
      "best acc: 79.100000\n",
      "Epoch: [84][0/391]\tTime 0.588 (0.588)\tData 0.521 (0.521)\tLoss 0.4227 (0.4227)\tPrec 89.062% (89.062%)\n",
      "Epoch: [84][100/391]\tTime 0.046 (0.051)\tData 0.002 (0.007)\tLoss 0.3957 (0.5241)\tPrec 84.375% (81.559%)\n",
      "Epoch: [84][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.5355 (0.5228)\tPrec 82.812% (81.681%)\n",
      "Epoch: [84][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.6804 (0.5205)\tPrec 74.219% (81.756%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.303 (0.303)\tLoss 0.6297 (0.6297)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.900% \n",
      "best acc: 79.100000\n",
      "Epoch: [85][0/391]\tTime 0.690 (0.690)\tData 0.628 (0.628)\tLoss 0.7339 (0.7339)\tPrec 73.438% (73.438%)\n",
      "Epoch: [85][100/391]\tTime 0.049 (0.055)\tData 0.002 (0.008)\tLoss 0.5964 (0.5051)\tPrec 77.344% (82.201%)\n",
      "Epoch: [85][200/391]\tTime 0.048 (0.052)\tData 0.002 (0.005)\tLoss 0.4183 (0.5023)\tPrec 88.281% (82.245%)\n",
      "Epoch: [85][300/391]\tTime 0.043 (0.050)\tData 0.002 (0.004)\tLoss 0.4780 (0.5084)\tPrec 84.375% (82.101%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.594 (0.594)\tLoss 0.6062 (0.6062)\tPrec 79.688% (79.688%)\n",
      " * Prec 77.760% \n",
      "best acc: 79.100000\n",
      "Epoch: [86][0/391]\tTime 0.773 (0.773)\tData 0.709 (0.709)\tLoss 0.5470 (0.5470)\tPrec 77.344% (77.344%)\n",
      "Epoch: [86][100/391]\tTime 0.046 (0.053)\tData 0.001 (0.009)\tLoss 0.5641 (0.5021)\tPrec 79.688% (82.287%)\n",
      "Epoch: [86][200/391]\tTime 0.048 (0.049)\tData 0.003 (0.006)\tLoss 0.4953 (0.5147)\tPrec 78.906% (81.860%)\n",
      "Epoch: [86][300/391]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.5444 (0.5134)\tPrec 80.469% (81.914%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.670 (0.670)\tLoss 0.6156 (0.6156)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.720% \n",
      "best acc: 79.100000\n",
      "Epoch: [87][0/391]\tTime 0.485 (0.485)\tData 0.449 (0.449)\tLoss 0.5867 (0.5867)\tPrec 81.250% (81.250%)\n",
      "Epoch: [87][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.006)\tLoss 0.5351 (0.5154)\tPrec 82.812% (81.699%)\n",
      "Epoch: [87][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.5388 (0.5129)\tPrec 82.812% (81.981%)\n",
      "Epoch: [87][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.5417 (0.5170)\tPrec 78.906% (81.982%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.388 (0.388)\tLoss 0.5064 (0.5064)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.180% \n",
      "best acc: 79.180000\n",
      "Epoch: [88][0/391]\tTime 0.607 (0.607)\tData 0.546 (0.546)\tLoss 0.5459 (0.5459)\tPrec 78.906% (78.906%)\n",
      "Epoch: [88][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.007)\tLoss 0.4391 (0.5119)\tPrec 85.156% (82.232%)\n",
      "Epoch: [88][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.005)\tLoss 0.6014 (0.5151)\tPrec 80.469% (82.187%)\n",
      "Epoch: [88][300/391]\tTime 0.041 (0.048)\tData 0.002 (0.004)\tLoss 0.5757 (0.5164)\tPrec 83.594% (81.920%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.451 (0.451)\tLoss 0.5583 (0.5583)\tPrec 83.594% (83.594%)\n",
      " * Prec 78.970% \n",
      "best acc: 79.180000\n",
      "Epoch: [89][0/391]\tTime 0.510 (0.510)\tData 0.461 (0.461)\tLoss 0.6370 (0.6370)\tPrec 76.562% (76.562%)\n",
      "Epoch: [89][100/391]\tTime 0.045 (0.051)\tData 0.002 (0.007)\tLoss 0.5946 (0.5223)\tPrec 76.562% (81.830%)\n",
      "Epoch: [89][200/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 0.5206 (0.5158)\tPrec 78.906% (82.016%)\n",
      "Epoch: [89][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.3802 (0.5195)\tPrec 87.500% (81.850%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.675 (0.675)\tLoss 0.5698 (0.5698)\tPrec 79.688% (79.688%)\n",
      " * Prec 79.630% \n",
      "best acc: 79.630000\n",
      "Epoch: [90][0/391]\tTime 0.561 (0.561)\tData 0.506 (0.506)\tLoss 0.4444 (0.4444)\tPrec 81.250% (81.250%)\n",
      "Epoch: [90][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.007)\tLoss 0.5138 (0.5174)\tPrec 82.812% (81.830%)\n",
      "Epoch: [90][200/391]\tTime 0.049 (0.050)\tData 0.002 (0.004)\tLoss 0.4884 (0.5183)\tPrec 86.719% (81.837%)\n",
      "Epoch: [90][300/391]\tTime 0.038 (0.048)\tData 0.002 (0.004)\tLoss 0.6854 (0.5147)\tPrec 77.344% (81.961%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.465 (0.465)\tLoss 0.4944 (0.4944)\tPrec 84.375% (84.375%)\n",
      " * Prec 77.870% \n",
      "best acc: 79.630000\n",
      "Epoch: [91][0/391]\tTime 0.679 (0.679)\tData 0.616 (0.616)\tLoss 0.5008 (0.5008)\tPrec 82.812% (82.812%)\n",
      "Epoch: [91][100/391]\tTime 0.049 (0.055)\tData 0.002 (0.008)\tLoss 0.6138 (0.5200)\tPrec 82.812% (81.683%)\n",
      "Epoch: [91][200/391]\tTime 0.043 (0.049)\tData 0.002 (0.005)\tLoss 0.6733 (0.5137)\tPrec 73.438% (82.035%)\n",
      "Epoch: [91][300/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.6508 (0.5121)\tPrec 79.688% (82.130%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.405 (0.405)\tLoss 0.5231 (0.5231)\tPrec 81.250% (81.250%)\n",
      " * Prec 79.090% \n",
      "best acc: 79.630000\n",
      "Epoch: [92][0/391]\tTime 0.642 (0.642)\tData 0.598 (0.598)\tLoss 0.4536 (0.4536)\tPrec 80.469% (80.469%)\n",
      "Epoch: [92][100/391]\tTime 0.042 (0.052)\tData 0.002 (0.008)\tLoss 0.4894 (0.5095)\tPrec 81.250% (81.969%)\n",
      "Epoch: [92][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.5768 (0.5161)\tPrec 82.031% (81.806%)\n",
      "Epoch: [92][300/391]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 0.5101 (0.5183)\tPrec 82.031% (81.699%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.311 (0.311)\tLoss 0.6115 (0.6115)\tPrec 81.250% (81.250%)\n",
      " * Prec 79.230% \n",
      "best acc: 79.630000\n",
      "Epoch: [93][0/391]\tTime 0.984 (0.984)\tData 0.930 (0.930)\tLoss 0.3929 (0.3929)\tPrec 88.281% (88.281%)\n",
      "Epoch: [93][100/391]\tTime 0.042 (0.055)\tData 0.002 (0.011)\tLoss 0.6436 (0.4985)\tPrec 76.562% (82.178%)\n",
      "Epoch: [93][200/391]\tTime 0.052 (0.051)\tData 0.002 (0.007)\tLoss 0.5479 (0.5048)\tPrec 82.031% (82.047%)\n",
      "Epoch: [93][300/391]\tTime 0.048 (0.050)\tData 0.002 (0.005)\tLoss 0.6910 (0.5088)\tPrec 78.125% (82.104%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.446 (0.446)\tLoss 0.6195 (0.6195)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.560% \n",
      "best acc: 79.630000\n",
      "Epoch: [94][0/391]\tTime 1.032 (1.032)\tData 0.975 (0.975)\tLoss 0.4206 (0.4206)\tPrec 84.375% (84.375%)\n",
      "Epoch: [94][100/391]\tTime 0.040 (0.052)\tData 0.002 (0.012)\tLoss 0.4965 (0.5101)\tPrec 81.250% (81.954%)\n",
      "Epoch: [94][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.007)\tLoss 0.5420 (0.5116)\tPrec 78.906% (81.895%)\n",
      "Epoch: [94][300/391]\tTime 0.039 (0.047)\tData 0.002 (0.005)\tLoss 0.5782 (0.5137)\tPrec 78.906% (81.839%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.5278 (0.5278)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.270% \n",
      "best acc: 79.630000\n",
      "Epoch: [95][0/391]\tTime 0.528 (0.528)\tData 0.453 (0.453)\tLoss 0.5674 (0.5674)\tPrec 78.906% (78.906%)\n",
      "Epoch: [95][100/391]\tTime 0.048 (0.052)\tData 0.002 (0.006)\tLoss 0.5341 (0.5128)\tPrec 80.469% (82.526%)\n",
      "Epoch: [95][200/391]\tTime 0.052 (0.049)\tData 0.002 (0.004)\tLoss 0.5316 (0.5162)\tPrec 85.156% (82.074%)\n",
      "Epoch: [95][300/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.4821 (0.5143)\tPrec 79.688% (82.065%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.516 (0.516)\tLoss 0.5332 (0.5332)\tPrec 82.031% (82.031%)\n",
      " * Prec 77.690% \n",
      "best acc: 79.630000\n",
      "Epoch: [96][0/391]\tTime 0.630 (0.630)\tData 0.542 (0.542)\tLoss 0.4690 (0.4690)\tPrec 83.594% (83.594%)\n",
      "Epoch: [96][100/391]\tTime 0.055 (0.051)\tData 0.003 (0.007)\tLoss 0.4204 (0.5117)\tPrec 86.719% (82.418%)\n",
      "Epoch: [96][200/391]\tTime 0.047 (0.048)\tData 0.003 (0.005)\tLoss 0.5395 (0.5100)\tPrec 82.031% (82.416%)\n",
      "Epoch: [96][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.004)\tLoss 0.4321 (0.5079)\tPrec 85.156% (82.345%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.452 (0.452)\tLoss 0.6743 (0.6743)\tPrec 78.906% (78.906%)\n",
      " * Prec 76.150% \n",
      "best acc: 79.630000\n",
      "Epoch: [97][0/391]\tTime 0.528 (0.528)\tData 0.467 (0.467)\tLoss 0.4919 (0.4919)\tPrec 83.594% (83.594%)\n",
      "Epoch: [97][100/391]\tTime 0.050 (0.050)\tData 0.003 (0.007)\tLoss 0.4761 (0.4925)\tPrec 85.156% (82.395%)\n",
      "Epoch: [97][200/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 0.4512 (0.5039)\tPrec 83.594% (82.202%)\n",
      "Epoch: [97][300/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.5644 (0.5055)\tPrec 78.906% (82.143%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.396 (0.396)\tLoss 0.5755 (0.5755)\tPrec 75.781% (75.781%)\n",
      " * Prec 77.420% \n",
      "best acc: 79.630000\n",
      "Epoch: [98][0/391]\tTime 0.588 (0.588)\tData 0.530 (0.530)\tLoss 0.4824 (0.4824)\tPrec 82.812% (82.812%)\n",
      "Epoch: [98][100/391]\tTime 0.047 (0.054)\tData 0.002 (0.007)\tLoss 0.5305 (0.5138)\tPrec 78.125% (81.900%)\n",
      "Epoch: [98][200/391]\tTime 0.043 (0.050)\tData 0.002 (0.005)\tLoss 0.5151 (0.5103)\tPrec 81.250% (82.156%)\n",
      "Epoch: [98][300/391]\tTime 0.051 (0.048)\tData 0.002 (0.004)\tLoss 0.5183 (0.5119)\tPrec 80.469% (82.044%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.312 (0.312)\tLoss 0.6790 (0.6790)\tPrec 76.562% (76.562%)\n",
      " * Prec 77.940% \n",
      "best acc: 79.630000\n",
      "Epoch: [99][0/391]\tTime 0.609 (0.609)\tData 0.552 (0.552)\tLoss 0.4692 (0.4692)\tPrec 84.375% (84.375%)\n",
      "Epoch: [99][100/391]\tTime 0.047 (0.052)\tData 0.002 (0.008)\tLoss 0.3412 (0.5132)\tPrec 89.844% (81.993%)\n",
      "Epoch: [99][200/391]\tTime 0.050 (0.049)\tData 0.003 (0.005)\tLoss 0.4183 (0.5121)\tPrec 89.062% (82.008%)\n",
      "Epoch: [99][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.004)\tLoss 0.5086 (0.5131)\tPrec 82.031% (81.987%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.366 (0.366)\tLoss 0.7695 (0.7695)\tPrec 74.219% (74.219%)\n",
      " * Prec 75.080% \n",
      "best acc: 79.630000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name) + \"_2bit\"\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7963/10000 (80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "PATH = \"result/resnet20_quant_2bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "1st convolution's input size: torch.Size([128, 3, 32, 32])\n",
      "2nd convolution's input size: torch.Size([128, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "print(\"1st convolution's input size:\", save_output.outputs[0][0].size())\n",
    "print(\"2nd convolution's input size:\", save_output.outputs[1][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405dc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d66166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "conv2= model.layer1[0].conv1\n",
    "#print(conv2.weight_q)\n",
    "#print(conv2.weight_quant.wgt_alpha.data.item())\n",
    "#print(conv2.weight_quant.wgt_alpha.data.item()/ (2**4 - 1))\n",
    "print(conv2.weight_quant.w_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0.,  0.,  0.],\n",
      "          [-0.,  0., -0.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1., -0.,  1.],\n",
      "          [-1.,  0.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-0., -0., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 2\n",
    "#weight_q = model.features[3].weight_q # quantized value is stored during the training\n",
    "\n",
    "weight_q = conv2.weight_q\n",
    "#print(conv2.weight)\n",
    "w_alpha = conv2.weight_quant.wgt_alpha.data.item()\n",
    "w_delta = w_alpha / (2**(w_bit - 1) - 1)\n",
    "weight_int = weight_q / w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(save_output.outputs[1][0])\n",
    "#print(conv2.act_alpha.data.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 0.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 0., 0., 1.],\n",
      "          [2., 2., 2.,  ..., 0., 0., 1.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 1., 3.,  ..., 2., 2., 2.],\n",
      "          [1., 0., 2.,  ..., 1., 1., 2.],\n",
      "          [1., 0., 2.,  ..., 0., 1., 2.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 1., 1.,  ..., 1., 1., 2.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 3., 1., 1.],\n",
      "          [3., 3., 3.,  ..., 3., 1., 1.],\n",
      "          [3., 3., 3.,  ..., 3., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 1.,  ..., 1., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 0., 0.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 1., 2.],\n",
      "          [2., 2., 1.,  ..., 1., 1., 1.],\n",
      "          [3., 2., 2.,  ..., 2., 1., 1.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 3.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 3., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[3., 3., 3.,  ..., 2., 2., 2.],\n",
      "          [3., 2., 2.,  ..., 2., 1., 1.],\n",
      "          [3., 3., 3.,  ..., 2., 1., 1.],\n",
      "          ...,\n",
      "          [3., 3., 3.,  ..., 2., 1., 1.],\n",
      "          [2., 2., 2.,  ..., 1., 0., 1.],\n",
      "          [2., 2., 2.,  ..., 2., 1., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 2.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 2.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 2.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 2\n",
    "x =  save_output.outputs[1][0] # input of the 2nd conv layer\n",
    "x_alpha  = conv2.act_alpha.data.item()\n",
    "x_delta = x_alpha / (2**x_bit - 1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q / x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 26.5919,  21.6060,  21.6060,  ...,  21.6060,  21.6060,  18.2820],\n",
      "          [ 29.9159,  24.9299,  24.9299,  ...,  24.9299,  24.9299,  21.6060],\n",
      "          [ 29.9159,  24.9299,  24.9299,  ...,  26.5919,  24.9299,  21.6060],\n",
      "          ...,\n",
      "          [ 41.5499,  38.2259,  36.5639,  ...,  24.9299,  16.6200,  14.9580],\n",
      "          [ 14.9580,  11.6340,  11.6340,  ...,  13.2960,  14.9580,  14.9580],\n",
      "          [ 19.9440,  19.9440,  19.9440,  ...,  26.5919,  24.9299,  24.9299]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-36.5639, -41.5499, -41.5499,  ..., -43.2119, -41.5499, -53.1839],\n",
      "          [-43.2119, -48.1979, -48.1979,  ..., -49.8599, -48.1979, -58.1699],\n",
      "          [-43.2119, -48.1979, -48.1979,  ..., -46.5359, -48.1979, -58.1699],\n",
      "          ...,\n",
      "          [  8.3100,  16.6200,  18.2820,  ..., -59.8319, -56.5079, -44.8739],\n",
      "          [  0.0000,   1.6620,   0.0000,  ..., -54.8459, -53.1839, -44.8739],\n",
      "          [-14.9580, -18.2820, -18.2820,  ..., -49.8599, -46.5359, -44.8739]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 84.7618,  74.7898,  74.7898,  ...,  78.1138,  74.7898,  63.1559],\n",
      "          [ 84.7618,  74.7898,  74.7898,  ...,  76.4519,  74.7898,  64.8179],\n",
      "          [ 84.7618,  74.7898,  74.7898,  ...,  76.4519,  74.7898,  64.8179],\n",
      "          ...,\n",
      "          [101.3818,  89.7478,  86.4238,  ...,  81.4378,  59.8319,  44.8739],\n",
      "          [ 74.7898,  59.8319,  59.8319,  ...,  64.8179,  53.1839,  44.8739],\n",
      "          [ 69.8039,  54.8459,  54.8459,  ...,  59.8319,  56.5079,  54.8459]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  3.3240,   3.3240,   3.3240,  ...,   1.6620,   3.3240,   8.3100],\n",
      "          [  6.6480,   6.6480,   6.6480,  ...,   4.9860,   6.6480,   9.9720],\n",
      "          [  6.6480,   6.6480,   6.6480,  ...,   8.3100,   6.6480,   9.9720],\n",
      "          ...,\n",
      "          [ -4.9860,  -8.3100,  -8.3100,  ...,   1.6620,   4.9860,   9.9720],\n",
      "          [-29.9159, -31.5779, -29.9159,  ...,  -6.6480,   3.3240,   9.9720],\n",
      "          [ -4.9860,  -3.3240,  -1.6620,  ...,  13.2960,  18.2820,  19.9440]]],\n",
      "\n",
      "\n",
      "        [[[ 16.6200,  16.6200,  26.5919,  ...,  23.2680,  14.9580,  14.9580],\n",
      "          [ 19.9440,  19.9440,  24.9299,  ...,  23.2680,  19.9440,  26.5919],\n",
      "          [ 19.9440,  19.9440,  24.9299,  ...,  21.6060,  21.6060,  31.5779],\n",
      "          ...,\n",
      "          [ 19.9440,  14.9580,  14.9580,  ...,  18.2820,  18.2820,  18.2820],\n",
      "          [ 19.9440,  14.9580,  13.2960,  ...,  14.9580,  14.9580,  14.9580],\n",
      "          [ 29.9159,  24.9299,  23.2680,  ...,  24.9299,  24.9299,  24.9299]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-36.5639, -23.2680,  -3.3240,  ..., -41.5499, -36.5639, -38.2259],\n",
      "          [-39.8879, -29.9159,  -4.9860,  ..., -46.5359, -41.5499, -38.2259],\n",
      "          [-39.8879, -29.9159,  -4.9860,  ..., -41.5499, -38.2259, -36.5639],\n",
      "          ...,\n",
      "          [-39.8879, -36.5639, -28.2539,  ..., -41.5499, -43.2119, -43.2119],\n",
      "          [-39.8879, -41.5499, -38.2259,  ..., -43.2119, -44.8739, -44.8739],\n",
      "          [-39.8879, -44.8739, -43.2119,  ..., -44.8739, -44.8739, -44.8739]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 51.5219,  56.5079,  76.4519,  ...,  69.8039,  54.8459,  53.1839],\n",
      "          [ 54.8459,  59.8319,  74.7898,  ...,  69.8039,  54.8459,  59.8319],\n",
      "          [ 54.8459,  59.8319,  74.7898,  ...,  66.4799,  59.8319,  73.1279],\n",
      "          ...,\n",
      "          [ 54.8459,  53.1839,  64.8179,  ...,  71.4659,  74.7898,  74.7898],\n",
      "          [ 54.8459,  48.1979,  51.5219,  ...,  53.1839,  54.8459,  54.8459],\n",
      "          [ 64.8179,  54.8459,  53.1839,  ...,  54.8459,  54.8459,  54.8459]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  9.9720,   4.9860,   0.0000,  ...,   9.9720,   4.9860,   4.9860],\n",
      "          [  9.9720,   4.9860,   0.0000,  ...,   8.3100,  11.6340,  19.9440],\n",
      "          [  9.9720,   4.9860,   0.0000,  ...,   3.3240,   9.9720,  16.6200],\n",
      "          ...,\n",
      "          [  9.9720,   1.6620,  -9.9720,  ...,  -9.9720, -11.6340, -11.6340],\n",
      "          [  9.9720,   6.6480,   0.0000,  ...,   1.6620,   0.0000,   0.0000],\n",
      "          [ 19.9440,  19.9440,  19.9440,  ...,  19.9440,  19.9440,  19.9440]]],\n",
      "\n",
      "\n",
      "        [[[ 16.6200,  11.6340,  11.6340,  ...,  11.6340,  11.6340,  11.6340],\n",
      "          [ 19.9440,  14.9580,  14.9580,  ...,  19.9440,  18.2820,  16.6200],\n",
      "          [ 19.9440,  14.9580,  14.9580,  ...,  31.5779,  26.5919,  21.6060],\n",
      "          ...,\n",
      "          [ 39.8879,  39.8879,  39.8879,  ...,  49.8599,  43.2119,  31.5779],\n",
      "          [ 39.8879,  39.8879,  38.2259,  ...,  44.8739,  39.8879,  31.5779],\n",
      "          [ 39.8879,  39.8879,  38.2259,  ...,  41.5499,  38.2259,  36.5639]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-36.5639, -39.8879, -39.8879,  ..., -39.8879, -39.8879, -39.8879],\n",
      "          [-39.8879, -44.8739, -44.8739,  ..., -44.8739, -46.5359, -46.5359],\n",
      "          [-39.8879, -44.8739, -44.8739,  ..., -38.2259, -44.8739, -46.5359],\n",
      "          ...,\n",
      "          [  9.9720,  19.9440,  19.9440,  ...,  -9.9720, -21.6060, -38.2259],\n",
      "          [  9.9720,  19.9440,  18.2820,  ...,   3.3240,  -8.3100, -29.9159],\n",
      "          [  9.9720,  19.9440,  16.6200,  ...,  14.9580,   8.3100, -18.2820]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 51.5219,  39.8879,  39.8879,  ...,  39.8879,  39.8879,  39.8879],\n",
      "          [ 54.8459,  44.8739,  44.8739,  ...,  49.8599,  48.1979,  46.5359],\n",
      "          [ 54.8459,  44.8739,  44.8739,  ...,  66.4799,  59.8319,  53.1839],\n",
      "          ...,\n",
      "          [ 99.7198,  89.7478,  89.7478,  ..., 116.3398, 101.3818,  78.1138],\n",
      "          [ 99.7198,  89.7478,  88.0858,  ..., 104.7058,  88.0858,  69.8039],\n",
      "          [ 99.7198,  89.7478,  86.4238,  ...,  91.4098,  78.1138,  68.1419]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  9.9720,  11.6340,  11.6340,  ...,  11.6340,  11.6340,  11.6340],\n",
      "          [  9.9720,   9.9720,   9.9720,  ...,  14.9580,  13.2960,  11.6340],\n",
      "          [  9.9720,   9.9720,   9.9720,  ...,  24.9299,  21.6060,  16.6200],\n",
      "          ...,\n",
      "          [ -4.9860,  -4.9860,  -4.9860,  ...,   0.0000,   1.6620,   3.3240],\n",
      "          [ -4.9860,  -4.9860,  -6.6480,  ...,  -9.9720,  -4.9860,   0.0000],\n",
      "          [ -4.9860,  -4.9860,  -4.9860,  ...,  -8.3100,   0.0000,   9.9720]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 43.2119,  39.8879,  36.5639,  ...,  41.5499,  31.5779,  19.9440],\n",
      "          [ 44.8739,  44.8739,  41.5499,  ...,  48.1979,  41.5499,  28.2539],\n",
      "          [ 39.8879,  39.8879,  43.2119,  ...,  39.8879,  34.9019,  26.5919],\n",
      "          ...,\n",
      "          [ 44.8739,  44.8739,  44.8739,  ...,  49.8599,  51.5219,  49.8599],\n",
      "          [ 44.8739,  44.8739,  44.8739,  ...,  49.8599,  51.5219,  51.5219],\n",
      "          [ 44.8739,  43.2119,  44.8739,  ...,  46.5359,  46.5359,  48.1979]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-19.9440, -24.9299, -23.2680,  ..., -16.6200, -34.9019, -41.5499],\n",
      "          [-18.2820, -26.5919, -29.9159,  ..., -13.2960, -38.2259, -46.5359],\n",
      "          [-18.2820, -26.5919, -28.2539,  ..., -11.6340, -33.2399, -48.1979],\n",
      "          ...,\n",
      "          [  0.0000,   4.9860,   4.9860,  ..., -11.6340, -18.2820, -19.9440],\n",
      "          [  0.0000,   4.9860,   4.9860,  ...,  -1.6620,  -4.9860,  -8.3100],\n",
      "          [  0.0000,   6.6480,   6.6480,  ...,   3.3240,   3.3240,   1.6620]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 98.0578,  89.7478,  89.7478,  ...,  91.4098,  76.4519,  58.1699],\n",
      "          [104.7058, 103.0438, 104.7058,  ..., 106.3678,  91.4098,  69.8039],\n",
      "          [103.0438, 103.0438, 104.7058,  ...,  99.7198,  86.4238,  69.8039],\n",
      "          ...,\n",
      "          [109.6918, 104.7058, 104.7058,  ..., 118.0018, 118.0018, 119.6637],\n",
      "          [109.6918, 104.7058, 104.7058,  ..., 111.3538, 114.6778, 118.0018],\n",
      "          [109.6918, 103.0438, 103.0438,  ..., 106.3678, 106.3678, 108.0298]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[ 11.6340,  18.2820,  19.9440,  ...,  18.2820,  19.9440,  16.6200],\n",
      "          [  3.3240,   4.9860,   3.3240,  ...,   4.9860,  11.6340,  13.2960],\n",
      "          [  3.3240,   4.9860,   4.9860,  ...,  -1.6620,   3.3240,   8.3100],\n",
      "          ...,\n",
      "          [ -1.6620,  -1.6620,  -1.6620,  ...,  -1.6620,   3.3240,   6.6480],\n",
      "          [ -1.6620,  -1.6620,  -1.6620,  ...,  -3.3240,  -3.3240,  -4.9860],\n",
      "          [ -1.6620,  -3.3240,  -3.3240,  ...,   0.0000,  -1.6620,  -1.6620]]],\n",
      "\n",
      "\n",
      "        [[[ 38.2259,  34.9019,  34.9019,  ...,  34.9019,  29.9159,  21.6060],\n",
      "          [ 41.5499,  38.2259,  38.2259,  ...,  38.2259,  33.2399,  24.9299],\n",
      "          [ 41.5499,  38.2259,  38.2259,  ...,  41.5499,  34.9019,  24.9299],\n",
      "          ...,\n",
      "          [ 38.2259,  34.9019,  36.5639,  ...,  34.9019,  29.9159,  23.2680],\n",
      "          [ 33.2399,  28.2539,  28.2539,  ...,  28.2539,  24.9299,  19.9440],\n",
      "          [ 33.2399,  29.9159,  29.9159,  ...,  29.9159,  29.9159,  28.2539]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-29.9159, -34.9019, -34.9019,  ..., -34.9019, -51.5219, -53.1839],\n",
      "          [-36.5639, -41.5499, -41.5499,  ..., -41.5499, -56.5079, -56.5079],\n",
      "          [-36.5639, -41.5499, -41.5499,  ..., -38.2259, -54.8459, -56.5079],\n",
      "          ...,\n",
      "          [-31.5779, -33.2399, -38.2259,  ..., -34.9019, -51.5219, -54.8459],\n",
      "          [-39.8879, -46.5359, -46.5359,  ..., -46.5359, -59.8319, -58.1699],\n",
      "          [-38.2259, -43.2119, -43.2119,  ..., -43.2119, -51.5219, -49.8599]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[109.6918, 104.7058, 104.7058,  ..., 104.7058,  88.0858,  66.4799],\n",
      "          [109.6918, 104.7058, 104.7058,  ..., 104.7058,  89.7478,  69.8039],\n",
      "          [109.6918, 104.7058, 104.7058,  ..., 108.0298,  91.4098,  69.8039],\n",
      "          ...,\n",
      "          [108.0298, 103.0438, 104.7058,  ..., 104.7058,  88.0858,  68.1419],\n",
      "          [101.3818,  94.7338,  94.7338,  ...,  94.7338,  81.4378,  64.8179],\n",
      "          [ 96.3958,  89.7478,  89.7478,  ...,  89.7478,  81.4378,  69.8039]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  1.6620,   1.6620,   1.6620,  ...,   1.6620,   8.3100,   9.9720],\n",
      "          [  4.9860,   4.9860,   4.9860,  ...,   4.9860,   9.9720,   9.9720],\n",
      "          [  4.9860,   4.9860,   4.9860,  ...,   8.3100,  11.6340,   9.9720],\n",
      "          ...,\n",
      "          [  3.3240,   3.3240,   3.3240,  ...,   1.6620,   8.3100,   9.9720],\n",
      "          [ -3.3240,  -4.9860,  -4.9860,  ...,  -4.9860,   1.6620,   4.9860],\n",
      "          [  1.6620,   1.6620,   1.6620,  ...,   1.6620,   9.9720,  14.9580]]],\n",
      "\n",
      "\n",
      "        [[[ 36.5639,  36.5639,  36.5639,  ...,  29.9159,  29.9159,  31.5779],\n",
      "          [ 39.8879,  39.8879,  39.8879,  ...,  33.2399,  29.9159,  29.9159],\n",
      "          [ 39.8879,  39.8879,  39.8879,  ...,  29.9159,  28.2539,  33.2399],\n",
      "          ...,\n",
      "          [ 19.9440,  14.9580,  14.9580,  ...,  14.9580,  16.6200,  16.6200],\n",
      "          [ 19.9440,  14.9580,  14.9580,  ...,  11.6340,  11.6340,  13.2960],\n",
      "          [ 29.9159,  24.9299,  24.9299,  ...,  24.9299,  24.9299,  24.9299]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[-11.6340, -14.9580, -14.9580,  ..., -23.2680, -19.9440, -19.9440],\n",
      "          [-14.9580, -19.9440, -19.9440,  ..., -34.9019, -28.2539, -26.5919],\n",
      "          [-14.9580, -19.9440, -19.9440,  ..., -38.2259, -29.9159, -26.5919],\n",
      "          ...,\n",
      "          [-36.5639, -43.2119, -43.2119,  ..., -33.2399, -34.9019, -39.8879],\n",
      "          [-39.8879, -44.8739, -44.8739,  ..., -39.8879, -39.8879, -41.5499],\n",
      "          [-39.8879, -44.8739, -44.8739,  ..., -44.8739, -44.8739, -44.8739]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 81.4378,  84.7618,  84.7618,  ...,  76.4519,  76.4519,  78.1138],\n",
      "          [ 84.7618,  89.7478,  89.7478,  ...,  78.1138,  74.7898,  76.4519],\n",
      "          [ 84.7618,  89.7478,  89.7478,  ...,  78.1138,  71.4659,  74.7898],\n",
      "          ...,\n",
      "          [ 58.1699,  49.8599,  49.8599,  ...,  46.5359,  48.1979,  49.8599],\n",
      "          [ 54.8459,  44.8739,  44.8739,  ...,  39.8879,  39.8879,  41.5499],\n",
      "          [ 64.8179,  54.8459,  54.8459,  ...,  54.8459,  54.8459,  54.8459]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  9.9720,  11.6340,  11.6340,  ...,   8.3100,   6.6480,   6.6480],\n",
      "          [  9.9720,   9.9720,   9.9720,  ...,  11.6340,   6.6480,   4.9860],\n",
      "          [  9.9720,   9.9720,   9.9720,  ...,   8.3100,   8.3100,  11.6340],\n",
      "          ...,\n",
      "          [  6.6480,   4.9860,   4.9860,  ...,   1.6620,   1.6620,   1.6620],\n",
      "          [  9.9720,   9.9720,   9.9720,  ...,  11.6340,  11.6340,  11.6340],\n",
      "          [ 19.9440,  19.9440,  19.9440,  ...,  19.9440,  19.9440,  19.9440]]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int =  conv_int(x_int)\n",
    "output_recovered = output_int * x_delta * w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "#conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "#conv_ref.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "#output_ref = conv_ref(x)\n",
    "#print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  31.3361,   25.7512,   26.3097,  ...,   26.8682,   28.2257,\n",
      "             22.8780],\n",
      "          [  41.2339,   35.6490,   33.1746,  ...,   36.2075,   35.6490,\n",
      "             32.7758],\n",
      "          [  41.2339,   35.6490,   35.6490,  ...,   36.7660,   35.6490,\n",
      "             32.7758],\n",
      "          ...,\n",
      "          [  68.3694,   62.5228,   57.1892,  ...,   34.0343,   34.1165,\n",
      "             37.0579],\n",
      "          [  20.4976,   15.2951,   14.1781,  ...,   20.4259,   29.6345,\n",
      "             34.5834],\n",
      "          [  24.7818,   31.2832,   31.2832,  ...,   42.8110,   45.2855,\n",
      "             45.2855]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -56.4604,  -66.9167,  -62.7668,  ...,  -60.6104,  -65.5592,\n",
      "            -76.1307],\n",
      "          [ -68.5922,  -83.4389,  -80.4059,  ...,  -76.0155,  -81.5229,\n",
      "            -91.6665],\n",
      "          [ -68.5922,  -83.4389,  -83.4389,  ...,  -80.4059,  -83.4389,\n",
      "            -91.6665],\n",
      "          ...,\n",
      "          [  30.2850,   32.4897,   21.1068,  ...,  -77.5467,  -88.0791,\n",
      "            -93.9831],\n",
      "          [   8.1971,    3.0563,   -2.2319,  ...,  -73.3550,  -85.5979,\n",
      "            -93.9831],\n",
      "          [ -28.0198,  -36.9903,  -37.7546,  ...,  -81.0560,  -90.3885,\n",
      "            -98.4094]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 100.4072,   95.2258,   94.4268,  ...,   91.7119,   93.3099,\n",
      "             86.0119],\n",
      "          [ 108.0711,  106.1631,  104.2471,  ...,  101.5322,  104.2471,\n",
      "             97.9355],\n",
      "          [ 108.0711,  106.1631,  106.1631,  ...,  105.9226,  106.1631,\n",
      "             97.9355],\n",
      "          ...,\n",
      "          [ 190.0332,  183.2488,  165.7576,  ...,  100.7858,   92.7724,\n",
      "             94.1138],\n",
      "          [ 125.6299,  109.7336,  101.7576,  ...,   83.1455,   86.5483,\n",
      "             93.9831],\n",
      "          [  93.4507,   81.0121,   79.5424,  ...,   91.0538,   99.0081,\n",
      "            107.0290]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[  13.2609,   16.5343,   15.4173,  ...,   12.9429,   16.5343,\n",
      "             20.1325],\n",
      "          [  15.4948,   15.4948,   14.9363,  ...,   16.8523,   17.4108,\n",
      "             18.1067],\n",
      "          [  15.4948,   15.4948,   15.4948,  ...,   18.5278,   15.4948,\n",
      "             18.1067],\n",
      "          ...,\n",
      "          [ -39.4165,  -48.3069,  -40.6731,  ...,    6.7612,   16.2339,\n",
      "             22.9250],\n",
      "          [ -68.7920,  -73.9386,  -65.8021,  ...,   -2.0081,   14.9155,\n",
      "             25.3994],\n",
      "          [  -3.3610,    6.2400,    9.3397,  ...,   29.9372,   34.3553,\n",
      "             33.7577]]],\n",
      "\n",
      "\n",
      "        [[[  24.8938,   34.0032,   70.2512,  ...,   26.7275,   20.2561,\n",
      "             22.8013],\n",
      "          [  39.7405,   48.8499,   73.5701,  ...,   34.1549,   31.7795,\n",
      "             32.3656],\n",
      "          [  39.7405,   48.8499,   73.5701,  ...,   32.1574,   38.2832,\n",
      "             44.6947],\n",
      "          ...,\n",
      "          [  39.7405,   34.5834,   23.2183,  ...,   24.7064,   27.1809,\n",
      "             24.7064],\n",
      "          [  39.7405,   34.5834,   27.1601,  ...,   29.6345,   29.6345,\n",
      "             29.6345],\n",
      "          [  47.8374,   42.8110,   35.3877,  ...,   32.9132,   32.9132,\n",
      "             32.9132]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -66.6336,  -37.3973,   20.5030,  ...,  -60.7510,  -61.1054,\n",
      "            -64.1383],\n",
      "          [ -81.4802,  -61.3428,    9.7608,  ...,  -70.7195,  -76.7510,\n",
      "            -77.5999],\n",
      "          [ -81.4802,  -61.3428,    9.7608,  ...,  -63.3393,  -75.9244,\n",
      "            -77.2220],\n",
      "          ...,\n",
      "          [ -81.4802,  -95.6782,  -68.5542,  ...,  -66.4322,  -65.0747,\n",
      "            -62.0418],\n",
      "          [ -81.4802, -101.1384,  -85.4220,  ...,  -75.9130,  -73.9970,\n",
      "            -73.9970],\n",
      "          [ -81.2189, -103.3583,  -95.9350,  ...,  -88.5116,  -88.5116,\n",
      "            -88.5116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  82.1047,  125.5632,  193.5035,  ...,   91.7990,   79.0576,\n",
      "             78.6797],\n",
      "          [  96.9514,  146.1577,  204.2457,  ...,   95.3628,   88.9837,\n",
      "             96.4845],\n",
      "          [  96.9514,  146.1577,  204.2457,  ...,   88.1799,   95.1371,\n",
      "            110.9654],\n",
      "          ...,\n",
      "          [  96.9514,  111.8224,  107.9385,  ...,   91.3435,   90.5446,\n",
      "             88.6286],\n",
      "          [  96.9514,  106.3622,   97.6089,  ...,   83.2789,   81.3630,\n",
      "             81.3630],\n",
      "          [ 105.0484,  111.9779,  104.5546,  ...,   97.1312,   97.1312,\n",
      "             97.1312]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[  23.0556,   14.7983,    4.9460,  ...,   16.1591,   16.5094,\n",
      "             22.0876],\n",
      "          [  23.0556,    9.0505,   -2.4774,  ...,    9.3300,   19.1738,\n",
      "             28.8587],\n",
      "          [  23.0556,    9.0505,   -2.4774,  ...,   11.6521,   21.0190,\n",
      "             21.9622],\n",
      "          ...,\n",
      "          [  23.0556,   14.9835,   -4.7325,  ...,   -5.9923,   -7.1093,\n",
      "             -7.6678],\n",
      "          [  23.0556,   20.4437,   14.3503,  ...,   18.7825,   20.6984,\n",
      "             20.6984],\n",
      "          [  31.1526,   31.2832,   31.2832,  ...,   36.2321,   36.2321,\n",
      "             36.2321]]],\n",
      "\n",
      "\n",
      "        [[[  24.8938,   19.7367,   19.7367,  ...,   19.7367,   19.7367,\n",
      "             19.7367],\n",
      "          [  39.7405,   34.5834,   34.5834,  ...,   38.1466,   36.9589,\n",
      "             35.7711],\n",
      "          [  40.2990,   34.5834,   34.5834,  ...,   50.0714,   45.2256,\n",
      "             40.2491],\n",
      "          ...,\n",
      "          [ 109.5198,  110.7764,  105.0798,  ...,   93.5735,   86.7238,\n",
      "             69.8725],\n",
      "          [ 106.6715,  107.9281,  102.2314,  ...,   93.9158,   85.6631,\n",
      "             69.3595],\n",
      "          [  91.4247,   93.6028,   90.7545,  ...,   80.8621,   71.9158,\n",
      "             66.6546]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -66.6336,  -81.4802,  -81.4802,  ...,  -81.4802,  -81.4802,\n",
      "            -81.4802],\n",
      "          [ -81.4802, -103.7503, -103.7503,  ..., -103.7503, -104.9380,\n",
      "           -104.9380],\n",
      "          [ -80.9218, -103.7503, -103.7503,  ...,  -96.0980, -102.1315,\n",
      "           -104.4961],\n",
      "          ...,\n",
      "          [  62.3806,   66.1490,   68.9973,  ...,   15.8440,   -2.1909,\n",
      "            -51.5337],\n",
      "          [  55.2598,   60.4524,   66.1490,  ...,   41.9754,   24.7395,\n",
      "            -37.8841],\n",
      "          [  45.7096,   53.2479,   57.5204,  ...,   52.2953,   37.4200,\n",
      "            -31.1601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  82.1047,   81.4802,   81.4802,  ...,   81.4802,   81.4802,\n",
      "             81.4802],\n",
      "          [  96.9514,  103.7503,  103.7503,  ...,  107.3135,  106.1257,\n",
      "            104.9380],\n",
      "          [  97.5099,  103.7503,  103.7503,  ...,  122.8014,  116.7679,\n",
      "            110.6036],\n",
      "          ...,\n",
      "          [ 222.4710,  222.4722,  211.0789,  ...,  183.1955,  165.1741,\n",
      "            139.1448],\n",
      "          [ 232.4402,  226.7447,  211.0789,  ...,  185.8542,  164.4926,\n",
      "            137.7820],\n",
      "          [ 218.6175,  210.9952,  196.7536,  ...,  169.0837,  149.2171,\n",
      "            130.7775]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[  23.0556,   30.4790,   30.4790,  ...,   30.4790,   30.4790,\n",
      "             30.4790],\n",
      "          [  23.0556,   23.0556,   23.0556,  ...,   26.6188,   25.4311,\n",
      "             24.2433],\n",
      "          [  23.6141,   23.0556,   23.0556,  ...,   37.3559,   33.6977,\n",
      "             28.7213],\n",
      "          ...,\n",
      "          [   3.0848,    1.6606,   -2.6119,  ...,   16.6705,   19.6351,\n",
      "             21.4068],\n",
      "          [  -8.3085,   -6.8844,   -8.3085,  ...,   -4.2870,    1.1732,\n",
      "              8.8583],\n",
      "          [ -26.4037,  -22.6338,  -18.3613,  ...,  -15.7747,   -8.8196,\n",
      "              9.1088]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  44.8174,   35.4005,   34.8979,  ...,   38.5110,   30.5547,\n",
      "             20.7923],\n",
      "          [  58.5471,   59.5865,   56.8608,  ...,   61.8980,   55.0587,\n",
      "             44.8684],\n",
      "          [  43.3824,   43.0644,   48.8789,  ...,   40.6007,   37.3529,\n",
      "             35.2111],\n",
      "          ...,\n",
      "          [  77.9405,   81.2914,   81.2914,  ...,   87.8383,   89.9947,\n",
      "             85.8448],\n",
      "          [  78.4990,   82.9669,   83.5254,  ...,   87.5978,   92.5467,\n",
      "             94.4626],\n",
      "          [  72.5510,   75.3889,   77.5774,  ...,   75.0255,   75.5840,\n",
      "             78.0584]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -38.1637,  -49.4298,  -44.1690,  ...,  -37.9636,  -55.2269,\n",
      "            -66.5097],\n",
      "          [ -38.9627,  -53.2617,  -49.9836,  ...,  -36.5953,  -60.5496,\n",
      "            -76.9020],\n",
      "          [ -26.2165,  -36.4323,  -35.4441,  ...,  -17.1177,  -40.4469,\n",
      "            -64.7171],\n",
      "          ...,\n",
      "          [  20.5153,   28.4971,   28.4971,  ...,   -8.5900,  -23.1403,\n",
      "            -25.3588],\n",
      "          [  21.0738,   30.1726,   30.1726,  ...,   14.3827,    5.5911,\n",
      "              1.0161],\n",
      "          [  19.3983,   30.1271,   30.1271,  ...,   22.1132,   21.5547,\n",
      "             18.5218]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 116.9881,  112.3093,  115.1468,  ...,  112.5715,  100.8156,\n",
      "             86.2464],\n",
      "          [ 125.9131,  126.8084,  132.1870,  ...,  128.0137,  115.6326,\n",
      "            101.3679],\n",
      "          [ 110.9331,  109.9791,  117.0332,  ...,  103.7610,   93.8544,\n",
      "             87.2104],\n",
      "          ...,\n",
      "          [ 180.1440,  181.6565,  181.6565,  ...,  182.7768,  182.4992,\n",
      "            184.2710],\n",
      "          [ 180.7025,  183.3320,  183.8905,  ...,  185.6622,  189.4274,\n",
      "            194.0583],\n",
      "          [ 174.7546,  176.3125,  177.4295,  ...,  174.2736,  175.3906,\n",
      "            177.3065]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[  14.2816,   19.8556,   22.3301,  ...,   22.7148,   24.1430,\n",
      "             23.3305],\n",
      "          [   3.5477,    1.6984,    0.5256,  ...,    0.9662,    6.6110,\n",
      "             11.6905],\n",
      "          [   7.7535,   10.2279,    8.4966,  ...,   -0.8535,    4.2328,\n",
      "             12.8398],\n",
      "          ...,\n",
      "          [  -5.0485,   -4.4900,   -4.4900,  ...,   11.6333,   20.0665,\n",
      "             23.4175],\n",
      "          [  -4.4900,   -3.3730,   -2.8145,  ...,   -3.8648,    0.9103,\n",
      "              0.6698],\n",
      "          [ -10.9964,  -12.6264,  -12.0680,  ...,  -10.1974,  -12.1134,\n",
      "            -11.5549]]],\n",
      "\n",
      "\n",
      "        [[[  49.7492,   47.2640,   47.2640,  ...,   47.2640,   38.1457,\n",
      "             31.4810],\n",
      "          [  58.9706,   56.4853,   56.4853,  ...,   56.4853,   45.9429,\n",
      "             42.8029],\n",
      "          [  60.3948,   56.4853,   56.4853,  ...,   58.1608,   46.5014,\n",
      "             42.8029],\n",
      "          ...,\n",
      "          [  51.5365,   47.0685,   49.5430,  ...,   42.9915,   35.2973,\n",
      "             37.4802],\n",
      "          [  40.8755,   34.7321,   34.7321,  ...,   34.3690,   29.9510,\n",
      "             34.1832],\n",
      "          [  35.4860,   31.5766,   31.5766,  ...,   32.6936,   32.0044,\n",
      "             40.0961]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -37.9340,  -44.8655,  -44.8655,  ...,  -44.8655,  -66.0213,\n",
      "            -76.9942],\n",
      "          [ -42.5757,  -52.8474,  -52.8474,  ...,  -52.8474,  -76.8514,\n",
      "            -92.2147],\n",
      "          [ -43.9999,  -54.2715,  -54.2715,  ...,  -52.5961,  -80.5654,\n",
      "            -95.0631],\n",
      "          ...,\n",
      "          [ -45.5574,  -51.0648,  -54.6562,  ...,  -45.3573,  -68.8696,\n",
      "            -87.2658],\n",
      "          [ -55.5327,  -63.5145,  -64.0730,  ...,  -56.4496,  -75.7533,\n",
      "            -89.4412],\n",
      "          [ -56.0912,  -62.3976,  -62.3976,  ...,  -57.8738,  -67.7520,\n",
      "            -80.6201]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 132.9548,  128.1581,  128.1581,  ...,  128.1581,  107.0024,\n",
      "             91.4431],\n",
      "          [ 143.9076,  142.4509,  142.4509,  ...,  142.4509,  118.4469,\n",
      "            105.5467],\n",
      "          [ 152.4525,  150.9959,  150.9959,  ...,  152.6714,  124.7020,\n",
      "            108.3950],\n",
      "          ...,\n",
      "          [ 151.2735,  143.3772,  146.4101,  ...,  132.9224,  109.8507,\n",
      "            101.7148],\n",
      "          [ 138.3785,  130.1642,  130.7227,  ...,  117.4863,  101.0309,\n",
      "             96.9270],\n",
      "          [ 119.6132,  111.9573,  111.9573,  ...,  104.1663,   94.2881,\n",
      "             96.8919]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[   9.4290,   12.7691,   12.7691,  ...,   12.7691,   16.8051,\n",
      "             20.8883],\n",
      "          [  13.3943,   14.8184,   14.8184,  ...,   14.8184,   18.8545,\n",
      "             20.9550],\n",
      "          [  10.5459,    9.1218,    9.1218,  ...,   10.7972,   16.5646,\n",
      "             19.5309],\n",
      "          ...,\n",
      "          [   2.0569,    2.8558,    2.8558,  ...,   -0.5402,    9.6843,\n",
      "             16.6159],\n",
      "          [ -10.7606,  -12.4360,  -12.9945,  ...,   -7.2979,    1.4385,\n",
      "             10.9111],\n",
      "          [  -9.0851,   -7.9681,   -7.9681,  ...,   -1.3500,    7.8391,\n",
      "             19.4958]]],\n",
      "\n",
      "\n",
      "        [[[  39.4561,   37.7807,   40.0146,  ...,   32.2665,   33.3128,\n",
      "             34.4297],\n",
      "          [  44.4050,   42.7296,   45.5220,  ...,   40.5555,   36.0277,\n",
      "             37.7032],\n",
      "          [  44.4050,   42.7296,   44.4050,  ...,   35.5400,   35.4692,\n",
      "             41.8531],\n",
      "          ...,\n",
      "          [  29.8427,   29.6345,   29.6345,  ...,   29.6345,   32.1090,\n",
      "             32.1090],\n",
      "          [  29.8427,   24.6856,   24.6856,  ...,   19.7367,   19.7367,\n",
      "             22.2112],\n",
      "          [  47.8374,   42.8110,   42.8110,  ...,   42.8110,   42.8110,\n",
      "             42.8110]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[ -22.7282,  -28.2356,  -26.0016,  ...,  -35.5667,  -31.5865,\n",
      "            -31.0280],\n",
      "          [ -28.7940,  -36.2174,  -32.8665,  ...,  -49.1933,  -41.2438,\n",
      "            -39.5683],\n",
      "          [ -28.7940,  -36.2174,  -32.8665,  ...,  -53.9725,  -44.8352,\n",
      "            -41.4843],\n",
      "          ...,\n",
      "          [ -59.1894,  -72.8093,  -73.3678,  ...,  -58.2031,  -61.7945,\n",
      "            -67.8604],\n",
      "          [ -71.5825,  -88.9036,  -88.9036,  ...,  -81.4802,  -81.4802,\n",
      "            -83.9547],\n",
      "          [ -81.2189, -103.3583, -103.3583,  ..., -103.3583, -103.3583,\n",
      "           -103.3583]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  94.3685,   94.8494,   97.0834,  ...,   85.9844,   88.1476,\n",
      "             89.8230],\n",
      "          [  98.2004,  100.5973,  103.9482,  ...,   94.0261,   90.5445,\n",
      "             92.7785],\n",
      "          [  98.2004,  100.5973,  103.9482,  ...,   97.1298,   90.7850,\n",
      "             94.1359],\n",
      "          ...,\n",
      "          [  81.8851,   78.9168,   78.3583,  ...,   73.2464,   74.6039,\n",
      "             77.3188],\n",
      "          [  87.0536,   88.9036,   88.9036,  ...,   81.4802,   81.4802,\n",
      "             83.9547],\n",
      "          [ 105.0484,  111.9779,  111.9779,  ...,  111.9779,  111.9779,\n",
      "            111.9779]],\n",
      "\n",
      "         [[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000],\n",
      "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "              0.0000]],\n",
      "\n",
      "         [[  12.0408,   13.9568,   15.6323,  ...,   12.3521,   11.7228,\n",
      "             11.1644],\n",
      "          [  13.1578,   13.1578,   13.7163,  ...,   17.6857,   10.9239,\n",
      "             10.9239],\n",
      "          [  13.1578,   13.1578,   12.0408,  ...,   15.6323,   14.5153,\n",
      "             16.7493],\n",
      "          ...,\n",
      "          [  18.3264,   23.1446,   23.7031,  ...,   18.9172,   20.0342,\n",
      "             19.7937],\n",
      "          [  23.0556,   28.0045,   28.0045,  ...,   30.4790,   30.4790,\n",
      "             30.4790],\n",
      "          [  31.1526,   31.2832,   31.2832,  ...,   31.2832,   31.2832,\n",
      "             31.2832]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "#weight = model.features[3].weight\n",
    "#mean = weight.data.mean()\n",
    "#std = weight.data.std()\n",
    "#conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight_q)\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.8815, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
